[Spark](https://spark.apache.org)
=====
* [Spark Documentation](https://spark.apache.org/documentation.html)
* [Databricks Spark Knowledge Base](https://www.gitbook.com/book/databricks/databricks-spark-knowledge-base/details)
* [Spark Programming Guide](https://spark.apache.org/docs/latest/programming-guide.html)
* [advanced dependency management](http://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management)
* [Custom API Examples For Apache Spark - The examples are basic and only for newbies in Scala and Spark](https://github.com/HyukjinKwon/spark-custom-api)
* [Welcome to Spark Python API Docs!](https://spark.apache.org/docs/latest/api/python/index.html)
* [github.com/apache/spark](https://github.com/apache/spark)
* [**SparkTutorials.net - Apache Spark For the Common * Man!**](http://sparktutorials.net/)
* [sparkjava.com/tutorials](http://sparkjava.com/tutorials/)
* [learn hadoop spark by examples](https://www.java-success.com/category/tutorial/hadoop-tutorials/learn-hadoop-spark-by-examples/)
* [**Running Spark Korean**](https://github.com/skyer9/Running-Spark-ko) flintrock, pyspark, aws s3, spark sql, jupyter, hadoop, yarn, tuning
* [Spark 시작하기 (유용한 사이트 링크)](https://www.facebook.com/notes/%EC%8A%A4%EC%82%AC%EB%AA%A8-%ED%95%9C%EA%B5%AD-%EC%8A%A4%ED%8C%8C%ED%81%AC-%EC%82%AC%EC%9A%A9%EC%9E%90-%EB%AA%A8%EC%9E%84-%EC%9D%B8%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%BB%B4%ED%93%A8%ED%8C%85/spark-%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0-%EC%9C%A0%EC%9A%A9%ED%95%9C-%EC%82%AC%EC%9D%B4%ED%8A%B8-%EB%A7%81%ED%81%AC/775214279207144?hc_location=ufi)
* [Learning Spark With Scala](https://maheshkndpl.wordpress.com/2017/09/01/introduction-to-spark/)
* [Apache Spark Scala Tutorial For Korean](https://github.com/skyer9/spark-scala-tutorial-ko)
* [Apache Spark Tutorial 2018 | Spark Tutorial | Spark Tutorial for Beginners | Apache Spark Training](https://www.youtube.com/watch?v=X4R8GweypwQ)
* [Big Data and Hadoop Tutorial For Beginners | Hadoop Spark Tutorial For Beginners](https://www.youtube.com/playlist?list=PLEiEAq2VkUULWstQPgNEiAhRYOsV1gWIy)
* [Apache Spark Tutorial](https://www.youtube.com/playlist?list=PLEiEAq2VkUUKmozD5fU7Fvc-Wl5SmD7TP)
* [Apache Spark Tutorials](https://www.youtube.com/playlist?list=PLkz1SCf5iB4dXiPdFD4hXwheRGRwhmd6K)
* [Apache Spark 101](https://medium.com/plumbersofdatascience/apache-spark-101-971aaf5d4334)
* [(16) Learn Apache Spark ( Databricks ) - Step by Step Guide | LinkedIn](https://www.linkedin.com/pulse/learn-apache-spark-databricks-step-guide-deepak-rajak/)
* [Spark Internals](https://github.com/JerryLead/SparkInternals)
* [Introduction to Spark Internals](http://www.slideshare.net/michiard/introduction-to-spark-internals)
* [Start Your Journey with Apache Spark — Part 1](https://medium.com/expedia-group-tech/start-your-journey-with-apache-spark-part-1-3575b20ee088)
* [Start Your Journey with Apache Spark — Part 2](https://medium.com/expedia-group-tech/start-your-journey-with-apache-spark-part-2-682891efda4b)
* [Start your Journey with Apache Spark — Part 3](https://medium.com/expedia-group-tech/start-your-journey-with-apache-spark-part-3-1ae77c05187)
* [Getting started with Spark & batch processing frameworks | by Hoa Nguyen | Insight](https://blog.insightdatascience.com/getting-started-with-spark-and-batch-processing-frameworks-d9f80c146e13)
* Spark Internal
  * [Part 1. RDD의 내부 동작](https://medium.com/@leeyh0216/spark-internal-part-1-rdd%EC%9D%98-%EB%82%B4%EB%B6%80-%EB%8F%99%EC%9E%91-d50eb7a235e6)
  * [**Part 2. Spark의 메모리 관리(1)**](https://medium.com/@leeyh0216/spark-internal-part-2-spark%EC%9D%98-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B4%80%EB%A6%AC-1-c18e39af942e)
  * [**Part 2. Spark의 메모리 관리(2)**](https://medium.com/@leeyh0216/spark-internal-part-2-spark%EC%9D%98-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B4%80%EB%A6%AC-2-db1975b74d2f)
  * [Spark Internal Part 3. Spark SQL’s Catalyst Optimizer](https://medium.com/@leeyh0216/spark-sql-6dc3d645cc31)
* [52. Apache Spark Internal architecture jobs stages and tasks || Spark Cluster Architecture Explained - YouTube](https://www.youtube.com/watch?v=hBc9Y0EA3_4)
* [pubdata.tistory.com/category/Lecture_SPARK](http://pubdata.tistory.com/category/Lecture_SPARK)
* [Apache Spark - Executive Summary](https://www.linkedin.com/pulse/apache-spark-executive-summary-alan-brown)
* [Teach yourself Apache Spark – Guide for nerds!](https://www.linkedin.com/pulse/teach-yourself-apache-spark-guide-nerds-shrinath-parikh)
* [Apache Spark - cyber.dbguide.net](http://cyber.dbguide.net/lecture.php?action=view&no=154)
* [Stanford CS347 Guest Lecture: Apache Spark](http://www.slideshare.net/rxin/stanford-cs347-guest-lecture-apache-spark)
* [BerkeleyX: CS100.1x Introduction to Big Data with Apache Spark](https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/)
  * [mooc-setup](https://github.com/spark-mooc/mooc-setup)
  * [Spark로 빅데이터 입문, 1-2주차 노트](http://seoh.github.io/blog/2015/06/10/big-data-with-spark-1-2-week/)
  * [Spark로 빅데이터 입문, 3주차 노트](http://seoh.github.io/blog/2015/06/14/big-data-with-spark-3-week/)
* [bigdatauniversity.com](http://bigdatauniversity.com)
  * [Spark Fundamentals I](http://bigdatauniversity.com/bdu-wp/bdu-course/spark-fundamentals/)
  * [Spark Fundamentals II](http://bigdatauniversity.com/bdu-wp/bdu-course/spark-fundamentals-ii/)
* [Apache Spark Full Course | Spark Tutorial For Beginners | Complete Spark Tutorial | Simplilearn - YouTube](https://www.youtube.com/watch?v=xoxns-p98Oc)
* [Top 5 Online Courses to Learn Apache Spark in 2022 - Best of Lot](https://javarevisited.blogspot.com/2021/11/top-5-courses-to-learn-apache-spark-in.html)
* [Introduction to Spark](https://www.dataquest.io/mission/123/introduction-to-spark/)
* [Introduction to Apache Spark with Scala](https://medium.com/@Babatee760/introduction-to-apache-spark-with-scala-ed31d8300fe4)
* [Python and Bigdata - An Introduction to Spark (PySpark)](https://www.slideshare.net/hiteshnd/python-and-bigdata-an-introduction-to-spark-pyspark)
* [Spark Programming](http://www.slideshare.net/taewook/spark-programming)
* [Intro to Apache Spark Training - Part 1](https://www.youtube.com/watch?v=VWeWViFCzzg)
* Cloudera
  * [Cloudera Engineering Blog · Spark Posts](http://blog.cloudera.com/blog/category/spark/)
  * [**How-to: Tune Your Apache Spark Jobs (Part 1)**](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/)
  * [**How-to: Tune Your Apache Spark Jobs (Part 2)**](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/)
  * [LSA-ing Wikipedia with Apache Spark](http://www.slideshare.net/cloudera/lsaing-wikipedia-with-apache-spark)
  * [Making Apache Spark Testing Easy with Spark Testing Base](http://blog.cloudera.com/blog/2015/09/making-apache-spark-testing-easy-with-spark-testing-base/)
  * [Getting Apache Spark Customers to Production](http://www.slideshare.net/cloudera/getting-apache-spark-customers-to-production)
  * [Why Your Apache Spark Job is Failing](http://www.slideshare.net/cloudera/why-your-apache-spark-job-is-failing)
  * [How to use Apache Spark with CDP Operational Database Experience - Cloudera Blog](https://blog.cloudera.com/how-to-use-apache-spark-with-cdp-operational-database-experience/)
* [The Apache Spark @youtube](https://www.youtube.com/user/TheApacheSpark)
* [Apache spark 소개 및 실습](http://www.slideshare.net/KangDognhyun/apache-spark-70360736)
* [Spark 소개 1부](http://www.slideshare.net/brotherjinho/spark-1-48694544)
* [Spark 소개 2부](http://www.slideshare.net/brotherjinho/spark-2-52028665)
* [RE: ShootingStar TV 1회 - 아파치 스파크와 RDD](https://www.youtube.com/watch?v=nuZecG90tLs)
  * [스터디용 아파치 스파크 환경구성 - 윈도우](https://www.youtube.com/watch?v=Rh62AHznlnc)
  * [스터디용 아파치 스파크 환경구성 - 인텔리J](https://www.youtube.com/watch?v=SWCf2A9xZgs)
* [databricks](https://databricks.com/)
  * [sparkhub.databricks.com](http://sparkhub.databricks.com/)
  * [Examples for Learning Spark](https://github.com/databricks/learning-spark)
  * [Project Tungsten: Bringing Spark Closer to Bare Metal](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)
    * [Project Tungsten: Apache Spark](http://www.infoobjects.com/project-tungsten-apache-spark/)
    * [Deep Dive into Project Tungsten: Bringing Spark Closer to Bare Metal-(Josh Rosen, Databricks)](http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen)
    * [SPARK 성능의 핵심 PROJECT TUNGSTEN 톺아보기](https://younggyuchun.wordpress.com/2017/01/31/spark-%EC%84%B1%EB%8A%A5%EC%9D%98-%ED%95%B5%EC%8B%AC-project-tungsten-%ED%86%BA%EC%95%84%EB%B3%B4%EA%B8%B0/)
    * [Catalyst and Tungsten: Apache Spark's Speeding Engine | LinkedIn](https://www.linkedin.com/pulse/catalyst-tungsten-apache-sparks-speeding-engine-deepak-rajak/)
  * [Simplifying Big Data Analytics with Apache Spark](http://www.slideshare.net/databricks/bdtc2?ref=http%3A%2F%2Fwww.slideshare.net%2Fdatabricks%2Fslideshelf)
  * [Databricks Announces General Availability of Its Cloud Platform](http://insidebigdata.com/2015/06/15/databricks-announces-general-availability-of-its-cloud-platform/)
  * [A Deeper Understanding of Spark Internals - Aaron Davidson (Databricks)](https://www.youtube.com/watch?v=dmL0N3qfSc8)
  * [DEVOPS ADVANCED CLASS](http://training.databricks.com/devops.pdf)
  * [스파크의 사용 환경 내용 - data bricks](http://knight76.tistory.com/entry/%ED%8E%8C-%EC%8A%A4%ED%8C%8C%ED%81%AC%EC%9D%98-%EC%82%AC%EC%9A%A9-%ED%99%98%EA%B2%BD-%EB%B0%9C%ED%91%9C)
  * databricks community edition [Hands-On Training for Data Science and Machine Learning - YouTube](https://www.youtube.com/watch?v=3AdVRy1R_8s)
* [What is shuffle read & shuffle write in Apache Spark](http://stackoverflow.com/questions/27276884/what-is-shuffle-read-shuffle-write-in-apache-spark)
* [Spark Shuffle Partition과 최적화 – tech.kakao.com](https://tech.kakao.com/2021/10/08/spark-shuffle-partition/)
* [Scrap your MapReduce! (Or, Introduction to Apache Spark)](http://rahulkavale.github.io/blog/2014/11/16/scrap-your-map-reduce/)
* [Learning Spark](https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/)
* [Introduction to Data Science with Apache Spark](http://ko.hortonworks.com/blog/introduction-to-data-science-with-apache-spark/)
* [HPC is dying, and MPI is killing it](http://www.dursi.ca/hpc-is-dying-and-mpi-is-killing-it/)
* [Spark은 왜 이렇게 유명해지고 있을까?](http://www.slideshare.net/KSLUG/ss-47355270)
* [Analytics With Apache Spark Is Coming](http://www.vidyasource.com/blog/Data/Hadoop/Analytics/Programming/Scala/Java/Python/Architecture/2015/04/22/analytics-with-apache-spark-is-coming)
* [Interactive Analytics using Apache Spark](http://www.slideshare.net/differentsachin/interactive-analytics-using-apache-spark)
* bicdata
  * 고급 분석을 '현실'로 만드는 스파크 -> 머신런닝 알고리즘이 포함 있지만, 고급분석가의 관점으로는 기초적인 알고리즘만 포함
  * 모든 것을 더 편하게 만들어주는 스파크 -> M/R 형식의 프로그램은 많이 편해짐. MPI 방식은 지원하지 않음
  * 하나 이상의 언어를 말하는 스파크 -> scala, java, python을 지원하지만, scala에 최적화되어 있고 나머지 언어는 좀 불편
  * 더 빨리 결과를 도출하는 스파크 -> 성능 테스트를 해보면, SparkStream은 storm보다 느리고, SparkSQL은 Hive보다 느림. 일반적인 Spark 프로그램이 성능이 좋음
  * 하둡 개발업체를 가리지 않는 스파크 -> 오픈소스는 대부분 업체를 가리지 않고, 용도와 장단점이 다름
  * 실시간 고급 분석 -> 기존(하둡)보다는 빠른 고급분석(??)이기 하지만, 준실시간
* [VCNC가 Hadoop대신 Spark를 선택한 이유](http://engineering.vcnc.co.kr/2015/05/data-analysis-with-spark/)
* [(25) 라인플러스 게임보안개발실...스파크+메소스로 10분 당 15TB 처리](https://www.imaso.co.kr/news/article_view.php?article_idx=20150519094003)
* [bcho.tistory.com/tag/Apache Spark](http://bcho.tistory.com/tag/Apache%20Spark)
  * [Spark 노트](http://bcho.tistory.com/983)
  * [Apache Spark이 왜 인기가 있을까?](http://bcho.tistory.com/1023)
  * [Apache Spark 설치 하기](http://bcho.tistory.com/1024)
  * [Apache Spark 소개 - 스파크 스택 구조](http://bcho.tistory.com/1026)
  * [Apache Spark 클러스터 구조](http://bcho.tistory.com/1025)
  * [Apache Spark - RDD (Resilient Distributed DataSet) 이해하기 - #1/2](http://bcho.tistory.com/1027)
  * [Apache Spark RDD 이해하기 #2 - 스파크에서 함수 넘기기 (Passing function to Spark)](http://bcho.tistory.com/1028)
  * [Apache Spark(스파크) - RDD Persistence (스토리지 옵션에 대해서)](http://bcho.tistory.com/1029)
  * [Apache Spark - Key/Value Paris (Pair RDD)](http://bcho.tistory.com/1030)
  * [Apache Spark-Python vs Scala 성능 비교](http://bcho.tistory.com/1031)
* [**blog.madhukaraphatak.com**](http://blog.madhukaraphatak.com/)
  * [Introduction to Spark Data Source API - Part 1](http://blog.madhukaraphatak.com/introduction-to-spark-data-source-api-part-1/)
* [Spark Summit](https://spark-summit.org/)
  * [Using Cascading to Build Data-centric Applications on Spark](https://spark-summit.org/2014/talk/using-cascading-to-build-data-centric-applications-on-spark)
    * [Expressing ETL workflows via Cascading](https://tech.flipkart.com/expressing-etl-workflows-via-cascading-192eb5e7d85d)
  * [spark-summit.org/2015](https://spark-summit.org/2015/)
    * [Spark Summit 2015- Track A](http://livestream.com/fourstream/sparksummit2015-tracka)
    * [Spark Summit 2015- Track B](http://livestream.com/fourstream/sparksummit2015-trackb)
    * [Spark Summit 2015- Track C](http://livestream.com/fourstream/sparksummit2015-trackc)
  * [spark-summit.org/east-2016/schedule](https://spark-summit.org/east-2016/schedule/)
    * [Spark Summit East 2016 첫 날 덤프](https://www.facebook.com/notes/jong-wook-kim/spark-summit-east-2016-%EC%B2%AB-%EB%82%A0-%EB%8D%A4%ED%94%84/1004799559567616)
    * [Spark Summit East 2016 둘째 날 덤프](https://www.facebook.com/notes/jong-wook-kim/spark-summit-east-2016-%EB%91%98%EC%A7%B8-%EB%82%A0-%EB%8D%A4%ED%94%84/1006965639351008)
  * [spark-summit.org/2016/schedule](https://spark-summit.org/2016/)
    * [A Deep Dive into Structured Streaming](http://www.slideshare.net/databricks/a-deep-dive-into-structured-streaming)
    * [How-to: Analyze Fantasy Sports using Apache Spark and SQL](http://blog.cloudera.com/blog/2016/06/how-to-analyze-fantasy-sports-using-apache-spark-and-sql/)
  * [Spark Summit 2016 West Training](https://www.youtube.com/playlist?list=PLK3eYwzuIEnUwvKo8ssbWMGippbMLyAxR)
    * [2016-06-06 Spark Summit West](https://drive.google.com/folderview?id=0B09cDg18tuRhMFg3cmtseC1KQ0U&usp=drive_web)
    * [Training Apache Spark Essentials](https://www.youtube.com/watch?v=OheiUl_uXwo)
      * [Class Notes - SSW 2016 Spark Essentials](http://tinyurl.com/Spark-Essentials-TE1)
    * [Training Continues: Apache Spark Essentials](https://www.youtube.com/watch?v=fROnFlD3Isw)
  * [**Spark Summit Europe 2016 참관기**](http://d2.naver.com/helloworld/8852387)
  * [OrderedRDD: A Distributed Time Series Analysis Framework for Spark (Larisa Sawyer)](https://www.youtube.com/watch?v=x2iM5he2gAU)
  * [Just Enough Scala for Spark (Dean Wampler)](https://www.youtube.com/watch?v=LBoSgiLV_NQ)
  * [TensorFrames: Deep Learning with TensorFlow on Apache Spark (Tim Hunter)](https://www.youtube.com/watch?v=gXItObf-qaI)
  * [SPARK SUMMIT EAST 2017](https://spark-summit.org/east-2017/schedule/)
  * [SPARK SUMMIT 2017 DATA SCIENCE AND ENGINEERING AT SCALE](https://spark-summit.org/2017/schedule/)
  * [비트윈 데이터팀의 Spark Summit EU 2017 참가기](http://engineering.vcnc.co.kr/2017/12/spark-summit-eu-2017/)
  * [2018-spark-summit-ai-keynotes-2](https://databricks.com/sparkaisummit/north-america/2018-spark-summit-ai-keynotes-2)
  * [Netflix at Spark+AI Summit 2018](https://medium.com/netflix-techblog/netflix-at-spark-ai-summit-2018-5304749ed7fa)
* [Spark(1.2.1 -> 1.3.1) 을 위한 Mesos(0.18 -> 0.22.rc) - Upgrade](http://hoondongkim.blogspot.kr/2015/05/spark121-131-mesos018-021-upgrade.html)
* [RDDS ARE THE NEW BYTECODE OF APACHE SPARK](https://ogirardot.wordpress.com/2015/05/29/rdds-are-the-new-bytecode-of-apache-spark/)
* [Spark RDD Operations-Transformation & Action with Example](https://data-flair.training/blogs/spark-rdd-operations-transformations-actions/)
* [Microbenchmarking Big Data Solutions on the JVM – Part 1](http://www.autoletics.com/posts/microbenchmarking-big-data-solutions-on-the-jvm-part-1)
* [Spark, Mesos, Zeppelin, HDFS를 활용한 대용량 보안 데이터 분석](http://developers.linecorp.com/blog/ko/?p=123)
* [(Berkeley CS186 guest lecture) Big Data Analytics Systems: What Goes Around Comes Around](http://www.slideshare.net/rxin/2015-0409-cs186guestlecture)
* [IBM, 오픈소스 커뮤니티에 머신러닝 기술 기증](http://www.bloter.net/archives/230353)
* [Productionizing Spark and the Spark Job Server](http://www.slideshare.net/EvanChan2/productionizing-spark-and-the-spark-job-server)
* [is Hadoop dead and is it time to move to Spark](http://www.quora.com/Is-Hadoop-dead-and-is-it-time-to-move-to-Spark)
* [Spark + S3 + R3 을 이용한 데이터 분석 시스템 만들기 by VCNC](https://speakerdeck.com/vcnc/spark-plus-s3-plus-r3-eul-iyonghan-deiteo-bunseog-siseutem-mandeulgi)
* [Parallel Programming with Spark (Part 1 & 2) - Matei Zaharia](https://www.youtube.com/watch?v=7k4yDKBYOcw)
* [3 Methods for Parallelization in Spark](https://towardsdatascience.com/3-methods-for-parallelization-in-spark-6a1a4333b473)
* [Stream All the Things! Architectures for Data Sets that Never End](https://deanwampler.github.io/polyglotprogramming/papers/StreamAllTheThings.pdf)
  * 스트리밍 중심 응용 프로그램 및 데이터 플랫폼 구축
  * 서비스를 함께 연결하는 단순성을 보여줌으로써 이벤트 소싱 아키텍처에 대해 동기를 부여
  * 실시간 및 분석 사례에 대한 다양한 시스템(Akka, Spark, Flink 및 기타)의 절충에 대해 설명
* [Petabyte-Scale Text Processing with Spark](http://tech.grammarly.com/blog/posts/Petabyte-Scale-Text-Processing-with-Spark.html)
* [Combining Druid and Spark: Interactive and Flexible Analytics at Scale](https://www.linkedin.com/pulse/combining-druid-spark-interactive-flexible-analytics-scale-butani)
* [Interactive Audience Analytics With Spark and HyperLogLog](http://eugenezhulenev.com/blog/2015/07/15/interactive-audience-analytics-with-spark-and-hyperloglog/)
* [Apache Spark Creator Matei Zaharia Interview](http://softwareengineeringdaily.com/2015/08/03/apache-spark-creator-matei-zaharia-interview/)
* [New Developments in Spark](http://www.slideshare.net/databricks/new-developments-in-spark)
* [Spark와 Hadoop, 완벽한 조합 (한국어)](http://www.slideshare.net/pudidic/spark-hadoop)
* [Spark Architecture: Shuffle](http://0x0fff.com/spark-architecture-shuffle/)
* [Deep-dive into Spark Internals & Architecture](https://www.linkedin.com/pulse/deep-dive-spark-internals-architecture-jayvardhan-reddy-vanchireddy/)
* [Naytev Wants To Bring A Buzzfeed-Style Social Tool To Every Publisher With Spark](http://techcrunch.com/2015/09/30/naytev-wants-to-bring-a-buzzfeed-style-social-tool-to-every-publisher-with-spark/)
* [Spinning up a Spark Cluster on Spot Instances: Step by Step](http://insightdataengineering.com/blog/sparkdevops/)
* [Spark Meetup at Uber](http://www.slideshare.net/databricks/spark-meetup-at-uber)
* [Bay Area Apache Spark Meetup @ Intel](https://www.youtube.com/watch?v=5UGkJzMBrQU)
  * [Easy, scalable, fault tolerant stream processing with structured streaming - spark meetup at intel in santa clara](https://www.slideshare.net/julesdamji/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-spark-meetup-at-intel-in-santa-clara-73743007)
* [Can Apache Spark process 100 terabytes of data in interactive mode?](http://fullstackml.com/2015/10/12/can-apache-spark-process-100-terabytes-of-data-in-interactive-mode/)
* [넷플릭스 빅데이터 플랫폼 아파치 스팍 통합 경험기](http://www.slideshare.net/deview/262-netflix)
* [Succinct Spark from AMPLab: Queries on Compressed RDDs](https://databricks.com/blog/2015/11/10/succinct-spark-from-amplab-queries-on-compressed-rdds.html)
* [How-to: Build a Complex Event Processing App on Apache Spark and Drools](http://blog.cloudera.com/blog/2015/11/how-to-build-a-complex-event-processing-app-on-apache-spark-and-drools/)
* [Tuning Spark](http://spark.apache.org/docs/latest/tuning.html)
* [Tuning Java Garbage Collection for Spark Applications](https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html)
* [Improving Spark application performance](http://chapeau.freevariable.com/2014/09/improving-spark-application-performance.html)
* [Spark performance tuning eng](https://www.slideshare.net/haiteam/spark-performance-tuning-eng)
* [Spark performance tuning Part#2 병렬처리](https://www.slideshare.net/haiteam/spark-performance-tuning-128798607)
* [Spark performance tuning from the trenches](https://medium.com/teads-engineering/spark-performance-tuning-from-the-trenches-7cbde521cf60)
* [Spark tuning for Enterprise System Administrators](http://techsuppdiva.github.io/spark1.6.html)
* [SPARK 설정 Tuning 하기 : 네이버 블로그](https://blog.naver.com/gyrbsdl18/220880041737)
* [“Fast food” and tips for RDD](http://pl.postech.ac.kr/~maidinh/blog/?p=61)
* [스칼라ML - 스칼라를 이용한 기계학습 기초(+Spark)](http://psygrammer.github.io/ScalaML/)
* [Secondary Sorting in Spark](http://codingjunkie.net/spark-secondary-sort/)
* [Distributed computing with spark](http://www.slideshare.net/javiersantospaniego/distributed-computing-with-spark)
* [Comparing the Dataflow/Beam and Spark Programming Models](https://cloud.google.com/blog/big-data/2016/02/comparing-the-dataflowbeam-and-spark-programming-models#closeImage)
* [Apache Spark Architecture](http://www.slideshare.net/AGrishchenko/apache-spark-architecture)
* [Scala vs. Python for Apache Spark](https://www.dezyre.com/article/scala-vs-python-for-apache-spark/213)
* [Natural Language Processing With Apache Spark](https://dzone.com/articles/in-progress-natural-language-processing)
* [맵알, ‘아파치 스파크’ 교육 과정 무료로 공개](http://www.bloter.net/archives/248982)
* [Spark HDFS Integration](http://0x0fff.com/spark-hdfs-integration/)
* [spark textfile load file instead of lines](http://stackoverflow.com/questions/29643348/spark-textfile-load-file-instead-of-lines)
* [Reading Text Files by Lines](https://wiki.ufal.ms.mff.cuni.cz/spark:recipes:reading-text-files)
* [Evening w/ Martin Odersky! (Scala in 2016) +Spark Approximates +Twitter Algebird](https://www.youtube.com/watch?v=_-I_X-k3D8A)
* [ScalaJVMBigData-SparkLessons.pdf](deanwampler.github.io/polyglotprogramming/papers/ScalaJVMBigData-SparkLessons.pdf)
* [Introduction to Spark 2.0 : A Sneak Peek At Next Generation Spark](http://blog.madhukaraphatak.com/introduction-to-spark-2.0/)
  * [Spark Release 2.0.0](https://spark.apache.org/releases/spark-release-2-0-0.html)
  * [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframe-data-readerwriter-interface)
  * [A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets - When to use them and why](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)
  * [Introducing Apache Spark 2.0](https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html)
  * [Spark 2.0 Technical Preview: Easier, Faster, and Smarter](https://databricks.com/blog/2016/05/11/spark-2-0-technical-preview-easier-faster-and-smarter.html)
  * [Apache Spark 2.0 presented by Databricks co-founder Reynold Xin](https://www.brighttalk.com/webcast/12891/202021)
  * [APACHE SPARK 2.0 API IMPROVEMENTS: RDD, DATAFRAME, DATASET AND SQL](http://www.agildata.com/apache-spark-2-0-api-improvements-rdd-dataframe-dataset-sql/)
    * RDD 보다 DataFrame, DataSet 이 속도는 두배 이상, 메모리 사용량은 1/4 미만
    * 도저히 DataFrame, DataSet 을 쓸수 없는 데이타(예를들어 기본 API 가 제공하지 않는 변환작업을 해야 하거나, 데이타가 뉴스 본문같은 구조화 할수없는 데이타이거나)가 아니면 RDD 말고 DataFrame, DataSet 사용
    * RDD; 자유도가 높음(Programming)
    * DataFrame; 자유도가 낮음(SQL-like) 대신 데이터 저장공간, 병렬화, 메모리 사용, 복합 쿼리 실행 플랜 등 아주 여러 부분에서 최적화 작업이 가능하고, 많이 최적화 작업이 되어있음
  * [Spark 2.0 – Datasets and case classes](https://blog.codecentric.de/en/2016/07/spark-2-0-datasets-case-classes/)
  * [Apache Spark 2.0 Performance Improvements Investigated With Flame Graphs](http://db-blog.web.cern.ch/blog/luca-canali/2016-09-spark-20-performance-improvements-investigated-flame-graphs)
  * [Generating Flame Graphs for Apache Spark](https://gist.github.com/kayousterhout/7008a8ebf2babeedc7ce6f8723fd1bf4)
  * [Apache Spark 2.0 Tuning Guide](http://www.slideshare.net/jcmia1/apache-spark-20-tuning-guide)
  * [Using Apache Spark 2.0 to Analyze the City of San Francisco's Open Data](https://www.youtube.com/watch?v=K14plpZgy_c)
  * [Modern Spark DataFrame & Dataset | Apache Spark 2.0 Tutorial](https://www.youtube.com/watch?v=_1byVWTEK1s)
  * [Structuring Apache Spark 2.0: SQL, DataFrames, Datasets And Streaming - by Michael Armbrust](https://www.youtube.com/watch?v=1a4pgYzeFwE)
  * [Apache Spark 2.0: A Deep Dive Into Structured Streaming - by Tathagata Das](https://www.youtube.com/watch?v=rl8dIzTpxrI)
  * [Spark 2.0 - by Matei Zaharia](https://www.youtube.com/watch?v=RUTeY4E2MoQ)
  * [Spark 2.x Troubleshooting Guide](https://www.slideshare.net/jcmia1/a-beginners-guide-on-troubleshooting-spark-applications)
* [Introducing Apache Spark 2.1 Now available on Databricks](https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html)
* [What's New in the Upcoming Apache Spark 2.3 Release?](http://go.databricks.com/databricks-runtime-4-with-apache-spark2-3)
* [Introducing Stream-Stream Joins in Apache Spark 2.3](https://databricks.com/blog/2018/03/13/introducing-stream-stream-joins-in-apache-spark-2-3.html)
* [ORC improvement in Apache Spark 2.3](https://dataworkssummit.com/berlin-2018/session/orc-improvement-in-apache-spark-2-3/)
* [The easiest way to run Spark in production](https://dcos.io/)
* [Structuring Spark: DataFrames, Datasets, and Streaming by Michael Armbrust](http://www.slideshare.net/SparkSummit/structuring-spark-dataframes-datasets-and-streaming-by-michael-armbrust)
* [Spark Takes On Dataflow in Benchmark Test](http://www.datanami.com/2016/05/02/dataflow-tops-spark-benchmark-test/)
* [Stock inference engine using Spring XD, Apache Geode / GemFire and Spark ML Lib. http://pivotal-open-source-hub.github.io/StockInference-Spark](https://github.com/Pivotal-Open-Source-Hub/StockInference-Spark)
* [Learning Spark - 아키텍트를 꿈꾸는 사람들](http://d2.naver.com/news/8818403)
  * [2015_LearningSpark](https://github.com/andstudy/afternoon/wiki/2015_LearningSpark)
* [Tutorial: Spark-GPU Cluster Dev in a Notebook A tutorial on ad-hoc, distributed GPU development on any Macbook Pro](https://iamtrask.github.io/2014/11/22/spark-gpu/)
* [GPU Acceleration on Apache Spark™](http://www.spark.tc/gpu-acceleration-on-apache-spark-2/)
* [Spark에서 GPU를 사용해야하는 이유는 무엇입니까?](http://aitimes.org/archives/264)
* [Cluster - spark](http://www.slideshare.net/HyeonSeokChoi/cluster-spark)
* [Apache Spark Key Terms, Explained](https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html)
* [스파크 클라우데라 하둡 클러스터 원격 입출력 예제](http://blog.naver.com/hancury/220744753944)
* [이렇게 코딩 하면 안된다](https://github.com/jaeho-kang/deep-learning/blob/master/SPARK/Introduction%20to%20Apache%20Spark_%EC%A0%95%EB%A6%AC.md)
* [spark를 이용한 hadoop cluster 원격 입출력](http://blog.naver.com/hancury/220744753944)
* [Best Practices for Using Apache Spark on AWS](http://www.slideshare.net/AmazonWebServices/best-practices-for-using-apache-spark-on-aws)
* [Working effectively with Apache Spark on AWS - Singapore Apache Spark+AI Meetup](https://www.youtube.com/watch?v=C8QDCcweI1A)
* [How to export millions of records from Mysql to AWS S3?](https://towardsdatascience.com/how-to-export-millions-of-records-from-mysql-to-aws-s3-fe30e80832e4)
* [Build a Prediction Engine Using Spark, Kudu, and Impala](https://dzone.com/articles/how-to-build-a-prediction-engine-using-spark-kudu)
* [Deep Dive: Apache Spark Memory Management](https://www.youtube.com/watch?v=dPHrykZL8Cg)
* [Deep Dive: Apache Spark Memory Management](http://go.databricks.com/deep-dive-apache-spark-memory-management)
* [Apache Spark Memory Management: Deep Dive | LinkedIn](https://www.linkedin.com/pulse/apache-spark-memory-management-deep-dive-deepak-rajak/)
* [A Developer’s View into Spark's Memory Model - Wenchen Fan](https://www.youtube.com/watch?v=-Aq1LMpzaKw)
* option
  * spark.executor.cores; node의 코어수
  * spark.cores.max 전체 갯수
  * e.g.
    * worker node가 2개이고 각 node당 8core cpu인데 spark.cores.max를 8로 주면 1개의 노드만 동작
    * 두개의 node에서 동작하게 하려면 spark.cores.max를 16으로
* [Apache Spark @Scale: A 60 TB+ production use case](https://code.facebook.com/posts/1671373793181703)
* [How Do In-Memory Data Grids Differ from Spark?](https://www.scaleoutsoftware.com/technology/how-do-in-memory-data-grids-differ-from-spark/)
* [Spark에서의 Data Skew 문제](http://eminency.github.io/techinal/spark/2016/10/08/data-skew.html)
* [Skew Mitigation For Facebook PetabyteScale Joins - YouTube](https://www.youtube.com/watch?v=QxfFbu65Hn8)
* [처음해보는 스파크(spark)로 24시간안에 부동산 과열 분석해보기](http://angeliot.blogspot.com/2016/11/24-spark.html)
* [Intro to Apache Spark for Java and Scala Developers - Ted Malaska (Cloudera)](https://www.youtube.com/watch?v=x8xXXqvhZq8)
* [**Achieving a 300% speedup in ETL with Apache Spark**](http://blog.cloudera.com/blog/2016/12/achieving-a-300-speedup-in-etl-with-spark/)
  * Spark의 CSV 파일 작업에 대한 스니펫 소개
  * non-distributed version에 비해 Spark는 뛰어난 속도 향상 기능을 제공하며 Parquet과 같은 최적화된 형식으로 변환 할 수 있는 기능을 제공
* [Parsing CSV Files in Spark](https://www.youtube.com/watch?v=I3WzvSjYf3Q)
* [Diving into Spark and Parquet Workloads, by Example](https://db-blog.web.cern.ch/blog/luca-canali/2017-06-diving-spark-and-parquet-workloads-example)
* [parquet 사용 예제](http://knight76.tistory.com/entry/spark-parquet-%ED%85%8C%EC%8A%A4%ED%8A%B8)
* [Apache Spark에서 컬럼 기반 저장 포맷 Parquet(파케이) 제대로 활용하기](http://engineering.vcnc.co.kr/2018/05/parquet-and-spark/)
* [입 개발 Spark에서 Parquet 파일 Custom Schema 로 읽어들이기 | Charsyam's Blog](https://charsyam.wordpress.com/2020/10/30/%EC%9E%85-%EA%B0%9C%EB%B0%9C-spark%EC%97%90%EC%84%9C-parquet-%ED%8C%8C%EC%9D%BC-custom-schema-%EB%A1%9C-%EC%9D%BD%EC%96%B4%EB%93%A4%EC%9D%B4%EA%B8%B0/)
* [Writing parquet on HDFS using Spark Streaming](https://community.cloudera.com/t5/Community-Articles/Writing-parquet-on-HDFS-using-Spark-Streaming/ta-p/248462)
* [Experimenting with Neo4j and Apache Zeppelin (Neo4j)-\[:LOVES\]-(Zeppelin)](https://medium.com/apache-zeppelin-stories/experimenting-with-neo4j-and-apache-zeppelin-d80b7bec8fd2)
* [Time-Series Missing Data Imputation In Apache Spark](http://www.jowanza.com/post/154094307399/time-series-missing-data-imputation-in-apache)
* [Tempo: Distributed Time Series Analysis with Apache Spark™ and Delta Lake - YouTube](https://www.youtube.com/watch?v=Op8PbbRAQtw)
* [**Data Science How-To: Using Apache Spark for Sports Analytics**](https://content.pivotal.io/blog/how-data-science-assists-sports)
  * [Using Spark To Analyze the NBA and the 3-point Shot](https://github.com/crawles/spark-nba-analytics)
* [Hive on Spark: Getting Started](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started)
* [Working with UDFs in Apache Spark](http://blog.cloudera.com/blog/2017/02/working-with-udfs-in-apache-spark/)
  * Python, Java, Scala에서 Apache Spark의 UDF, UDAF를 사용하는 간단한 예제
* [Apache Spark은 어떻게 가장 활발한 빅데이터 프로젝트가 되었나](http://readme.skplanet.com/wp-content/uploads/%ED%8A%B8%EB%9E%991-3Apache-Spark%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EA%B0%80%EC%9E%A5-%ED%99%9C%EB%B0%9C%ED%95%9C-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%EA%B0%80-%EB%90%98%EC%97%88%EB%82%98.pdf)
* [Using Apache Spark for large-scale language model training](https://code.facebook.com/posts/678403995666478/using-apache-spark-for-large-scale-language-model-training/)
  * Facebook에서 ngram 모델의 traing pipeline을 Apach Hive에서 Apache Spark으로 전환 시도 중
  * 두 가지 솔루션에 대한 설명과 Spark DSL 과 Hive QL의 유연성 비교 및 성능 수치
* [Hive and Spark Integration Tutorial](https://www.youtube.com/watch?v=Tq_NL2HEdfM)
* [Working with multiple partition formats within a Hive table with Spark](https://blog.godatadriven.com/multiformat-spark-partition)
  * Hive는 파티션별로 다른 데이터 형식을 지원, 데이터를 쓰기 최적화된 형식에서 읽기 최적화된 형식으로 변환할 때 사용 가능
  * Spark에서 멀티 포맷 테이블을 쿼리할 때 실행 계획이 어떻게 동작하는지 내부 동작 방식에 대해 설명
* [On Spark, Hive, and Small Files: An In-Depth Look at Spark Partitioning Strategies](https://medium.com/airbnb-engineering/on-spark-hive-and-small-files-an-in-depth-look-at-spark-partitioning-strategies-a9a364f908)
* [Integrating Apache Hive with Apache Spark - Hive Warehouse Connector](https://community.hortonworks.com/articles/223626/integrating-apache-hive-with-apache-spark-hive-war.html)
* [How to access Hive from Spark2 on HDP3?](https://www.youtube.com/watch?v=IPT0TzsZE98)
* [WRITING TO A DATABASE FROM SPARK](http://bigdatums.net/2016/10/16/writing-to-a-database-from-spark/)
* [Processing Solr data with Apache Spark SQL in IBM IOP 4.3](https://developer.ibm.com/hadoop/2017/03/21/processing-solr-data-apache-spark-sql-ibm-iop-4-3/)
  * Apache Spark을 Apach Solr로 연결하는 방법 소개
* [Blacklisting in Apache Spark](https://blog.cloudera.com/blog/2017/04/blacklisting-in-apache-spark/)
* [Tracking the Money — Scaling Financial Reporting at Airbnb](https://medium.com/airbnb-engineering/tracking-the-money-scaling-financial-reporting-at-airbnb-6d742b80f040)
* [The Benefits of Migrating HPC Workloads To Apache Spark](https://hortonworks.com/blog/recent-improvements-apache-zeppelin-livy-integration/)
  * Spark 작업을 실행하기위한 Apache Zeppelin과 Livy 작업 서버 간의 통합에 대한 최근 개선 사항 설명
* [데이터분석 인프라 구축기 (1/4)](https://medium.com/@gamzabaw/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D-%EC%9D%B8%ED%94%84%EB%9D%BC-%EA%B5%AC%EC%B6%95%EA%B8%B0-1-4-fc1ff841dae9)
* [데이터분석 인프라 구축기 (2/4)](https://medium.com/@gamzabaw/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D-%EC%9D%B8%ED%94%84%EB%9D%BC-%EA%B5%AC%EC%B6%95%EA%B8%B0-2-4-616df0d52ac3)
* [데이터분석 인프라 구축기 (3/4)](https://medium.com/@gamzabaw/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D-%EC%9D%B8%ED%94%84%EB%9D%BC-%EA%B5%AC%EC%B6%95%EA%B8%B0-3-4-bb2326089ba5)
* [데이터분석 인프라 구축기 (4/4)](https://medium.com/@gamzabaw/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D-%EC%9D%B8%ED%94%84%EB%9D%BC-%EA%B5%AC%EC%B6%95%EA%B8%B0-4-4-b74ba73c426c)
* [zipWithIndex, for-yield 예제](http://knight76.tistory.com/entry/spark-zipWithIndex-foryield-%EC%98%88%EC%A0%9C)
* [Cloudera session seoul - Spark bootcamp](https://www.slideshare.net/SangbaeLim/cloudera-sessions-seoul-spark-bootcamp)
* [Benchmarking Big Data SQL Platforms in the Cloud](https://databricks.com/blog/2017/07/12/benchmarking-big-data-sql-platforms-in-the-cloud.html)
  * Vanilla Spark, Presto, Impala 보다 DataBricks 플랫폼이 더 빠르다는 주장
* [Building QDS: AIR Infrastructure](https://www.qubole.com/blog/building-qdsair-infrastructure/)
  * Qubole이 Data Platforms 2017 conference 발표한 Air라는 플랫폼에 대한 내용입니다.
* [스파크 스터디 ParkS](https://github.com/psygrammer/ParkS/)
  * [ParkS](https://docs.google.com/spreadsheets/d/1XSbyjPibkJiQPpi29y9oLlMD9ihHvazb7L0uIS5wGA8/edit#gid=959319708)
* [Cost Based Optimizer in Apache Spark 2.2](https://databricks.com/blog/2017/08/31/cost-based-optimizer-in-apache-spark-2-2.html)
  * Apache Spark 2.2의 Cost Based Optimizer와 TPC-DS benchmark에서 CBO 사용 여부에 관계없이 쿼리 수행 시간을 비교한 결과와 통계 정보 수집 방법 등에 대해 설명
* [Apache Spark Core-Deep Dive-Proper Optimization - Daniel Tomes, Databricks](https://www.youtube.com/watch?v=daXEp4HmS-E)
* [spark 프레임워크를 활용해 자바 기반 웹 애플리케이션 개발 맛보기](https://slipp.net/questions/548)
* [Bay Area Apache Spark Meetup at HPE/Aruba Networks Summary](https://databricks.com/blog/2017/09/22/bay-area-apache-spark-meetup-at-hpearuba-networks-summary.html)
  * Aruba에서 PySpark 및 GraphFrames의 Databricks를 사용한 데이터 상관 관계에 관한 프레젠테이션
* [Apache Spark Professional Training with Hands On Lab](http://www.hadoopexam.com/spark/training/Apache_Spark_professional_training_developer_certification_exam_dumps.html)
* [IBM Cloud 환경에서 DSX Spark를 사용한 데이터 분석 시작하기](https://developer.ibm.com/kr/developer-%EA%B8%B0%EC%88%A0-%ED%8F%AC%EB%9F%BC/2017/11/08/ibm-cloud-dsx-spark/)
* [Spark-overflow - A collection of Spark related information, solutions, debugging tips and tricks, etc. PR are always welcome! Share what you know about Apache Spark](https://github.com/AllenFang/spark-overflow)
* [Debugging a long-running Apache Spark application: A War Story](https://tech.channable.com/posts/2018-04-10-debugging-a-long-running-apache-spark-application.html)
  * 장기간 실행되는 Apache Spark 응용 프로그램의 성능 문제를 디버깅하는 방법 대해 설명
  * JVM 내부 (예 : 사용자 정의 클래스 로더 및 GC), Spark internal (예 : driver가 broadcast data를 정리하는 방법) 및 이러한 버그를 찾아 내고 확인하는 메트릭 및 모니터링 전략
* [A step-by-step guide for debugging memory leaks in Spark Applications | by Shivansh Srivastava | disney-streaming | Nov, 2020 | Medium](https://medium.com/disney-streaming/a-step-by-step-guide-for-debugging-memory-leaks-in-spark-applications-e0dd05118958)
* [A Deeper Understanding of Spark Internals - Aaron Davidson (Databricks)](https://www.youtube.com/watch?v=dmL0N3qfSc8)
* [Using Apache Spark to Analyze Large Neuroimaging Datasets](https://blog.dominodatalab.com/pca-on-very-large-neuroimaging-datasets-using-pyspark/)
* [Goal Based Data Production: The Spark of a Revolution - Sim Simeonov](https://www.youtube.com/watch?v=VR2lAMVD4_4)
* [Spark Job On Mesos - Log Handling](http://hoondongkim.blogspot.com/2016/02/spark-job-on-mesos-log-handling.html) programtic하게 log level별로 원하는 장소에 로그 남기기
* [How to log in Apache Spark](https://hackernoon.com/how-to-log-in-apache-spark-f4204fad78a) log4j
* [Spark: Web Server Logs Analysis with Scala](https://medium.com/analytics-vidhya/spark-web-server-logs-analysis-with-scala-74e0ece40a4e)
* [Top 5 Mistakes to Avoid When Writing Apache Spark Applications](https://intellipaat.com/blog/top-5-mistakes-writing-apache-spark-applications/)
* [Extreme Apache Spark: how in 3 months we created a pipeline that can process 2.5 billion rows a day](https://www.slideshare.net/jozefhabdank/extreme-apache-spark-how-in-3-months-we-created-a-pipeline-that-can-process-25-billion-rows-a-day)
* [Locality Sensitive Hashing By Spark](https://www.slideshare.net/SparkSummit/locality-sensitive-hashing-by-spark)
* [Partition Index - Selective Queries On Really Big Tables](https://medium.com/@adirmashiach/partition-index-selective-queries-on-really-big-tables-795fea737570) Hive, Impala, Spark 등으로 데이터를 조회할 때 전체 테이블을 검색하지 않도록 클라이언트와 데몬 사이에서 인덱스 맵을 만들고 유지 관리를 하며 쿼리를 파싱해주는 balancer를 구현
* Practical Apache Spark in 10 minutes
  * [Part 1 - Ubuntu installation](https://datascience-school.com/blog/practical-apache-spark-in-10-minutes-part-1-ubuntu-installation/)
  * [Part 2 - RDD](https://datascience-school.com/blog/practical-apache-spark-in-10-minutes-part-2-rdd/)
  * [Part 3 - Data Frames and SQL](https://datascience-school.com/blog/practical-apache-spark-in-10-minutes.-part-3-dataframes-and-sql/)
  * [Part 4 — MLlib](https://datascience-school.com/blog/practical-apache-spark-in-10-minutes.-part-4-mllib/)
  * [Part 5 - Streaming](https://datascience-school.com/blog/practical-apache-spark-in-10-minutes.-part-5-streaming/)
  * [Part 6 - GraphX](https://datascience-school.com/blog/practical-apache-spark-in-10-minutes.-part-6-graphx)
* partition의 개수가 지나치게 적게 잡혀서 worker 역시 부족하게 할당되면서 성능 하락 [problem e.g](https://www.facebook.com/groups/sparkkoreauser/permalink/1753810921347470/)
  * 다양한 경우에서 자주 발생
  * spark sql optimizer가 업그레이드 되는 게 가장 확실한 해법이지만 그걸 기다릴 수 없기 때문에 repartition을 사용해 강제로 partition 수를 증가

    ```
    val dataset: Dataset[XXX] = ...
    dataset.repartition(dataset.rdd.getNumPartitions * 2).map(YYY)...
    ```
* [Apache Spark Scheduler](https://databricks.com/session/apache-spark-scheduler)
* [Deep Dive into the Apache Spark Scheduler - Xingbo Jiang](https://www.youtube.com/watch?v=rpKjcMoega0)
* [Apache Spark: Scala vs. Java v. Python vs. R vs. SQL](https://mindfulmachines.io/blog/2018/6/apache-spark-scala-vs-java-v-python-vs-r-vs-sql26)
  * Scala, Java, Python, R 및 SQL에서 Apache Spark API의 차이점 설명
  * 예상대로, JVM 언어를 사용하면 성능 향상
* [Exploratory Data Analysis in Spark with Jupyter](http://blog.madhukaraphatak.com/exploratory-data-analysis-in-spark-with-jupyter/)
* [아파치 스팍 관련 문제점 이야기 + 자바로 게으른 초기화 (2018-07-06) 케빈TV Live](https://www.youtube.com/watch?v=YTF3CUzWDo8)
* [Working with Nested JSON Using Spark | Parsing Nested JSON File in Spark](https://www.youtube.com/watch?v=n3owBqa9ATQ)
* [Working with JSON in Apache Spark](https://medium.com/expedia-group-tech/working-with-json-in-apache-spark-1ecf553c2a8c)
* [practice - sc.textFile로 gzipped hdfs file을 읽을 경우 성능 저하 or job 실패](https://gist.github.com/hyunjun/8fb9629db2df065c4a8eb6fa54f516ff#file-unzip_gzipped_hdfs_file-md)
* [What’s new in Apache Spark 2.3 and Spark 2.4](https://www.slideshare.net/Hadoop_Summit/whats-new-in-apache-spark-23-and-spark-24)
* [What’s new in Spark 2.4!](https://medium.com/@akhilanand.bv/whats-new-in-spark-2-4-121162f1c385)
* [Uber’s Big Data Platform: 100+ Petabytes with Minute Latency](https://eng.uber.com/uber-big-data-platform/)
  * Uber가 Hadoop과 Spark을 이용하여 빅데이터를 수집, 관리, 분석하는 방법 정리
* [Spark study notes: core concepts visualized](https://medium.com/@pang.xin/spark-study-notes-core-concepts-visualized-5256c44e4090)
  * Spark과 YARN이 상호 작용하는 방식과 작업이 각 단계에서 어떻게 작동하는지 설명하는 기초 문서
* [Just Enough Spark! Core Concepts Revisited !! | LinkedIn](https://www.linkedin.com/pulse/just-enough-spark-core-concepts-revisited-deepak-rajak/)
* Python vs. Scala
  * [a comparison of the basic commands (Part I)](https://towardsdatascience.com/python-vs-scala-a-comparison-of-the-basic-commands-fae23b3ede23)
  * [Pandas vs. Spark: how to handle dataframes (Part II)](https://towardsdatascience.com/python-pandas-vs-scala-how-to-handle-dataframes-part-ii-d3e5efe8287d)
* [Points to remember while processing streaming timeseries data in order using Kafka and Spark](https://medium.com/@sathishjayaram/tips-for-processing-streaming-timeseries-data-in-order-using-kafka-and-spark-2e07cf5e4256)
* A Journey Into Big Data with Apache Spark
  * [Part 1](https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2)
  * [Part 2](https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-2-4511aa19a900)
* [Write to multiple outputs by key Spark - one Spark job](https://stackoverflow.com/questions/23995040/write-to-multiple-outputs-by-key-spark-one-spark-job)
* [Things I Wish I’d Known About Spark When I Started (One Year Later Edition)](https://medium.com/enigma-engineering/things-i-wish-id-known-about-spark-when-i-started-one-year-later-edition-d767430181ed)
* [Brian Clapper—Spark for Scala Developers](https://www.youtube.com/watch?v=zXipqCcVmQM)
* [Movie recommendation using Apache Spark](https://medium.com/@varunabhi86/movie-recommendation-using-apache-spark-1a41e24b94ba)
* [NPE from Spark App that extends scala.App](https://medium.com/@manuzhang/npe-from-spark-app-that-extends-scala-app-ef7378195850)
* [입 개발 spark-submit 시에 –properties-file 와 파라매터에서의 우선 순위](https://charsyam.wordpress.com/2019/03/10/%EC%9E%85-%EA%B0%9C%EB%B0%9C-spark-submit-%EC%8B%9C%EC%97%90-properties-file-%EC%99%80-%ED%8C%8C%EB%9D%BC%EB%A7%A4%ED%84%B0%EC%97%90%EC%84%9C%EC%9D%98-%EC%9A%B0%EC%84%A0-%EC%88%9C%EC%9C%84/)
* [Which Language to choose when working with Apache Spark](https://medium.com/@write2karanverma/which-language-to-choose-when-working-with-apache-spark-1cddc46e40fd)
* [Procesando Datos con Spark](https://medium.com/@crscardellino/procesando-datos-con-spark-48539d38e437)
* [Which Career Should I Choose — Hadoop Admin or Spark Developer?](https://medium.com/@etlhive/which-career-should-i-choose-hadoop-admin-or-spark-developer-391247e7d13e)
* [Efficient geospatial analysis with Spark](https://medium.com/@karijdempsey/efficient-geospatial-analysis-with-spark-363ba50c5248)
* [Dealing with null in Spark](https://www.mungingdata.com/apache-spark/dealing-with-null)
* ['.NET for Apache Spark' Debuts for C#/F# Big Data](https://visualstudiomagazine.com/articles/2019/04/25/net-spark.aspx)
* [Announcing Version 1.0 of .NET for Apache Spark | .NET Blog](https://devblogs.microsoft.com/dotnet/announcing-version-1-0-of-net-for-apache-spark/)
* [Parallel Cross Validation in Spark](http://blog.madhukaraphatak.com/parallel-cross-validation/)
* [Vedant Jain: Smart Streams: A Real-time framework for scoring Big and Fast Data | PyData Miami 2019](https://www.youtube.com/watch?v=C6bAr9f_myQ)
* [Jakub Hava: Productionizing H2O Models with Apache Spark | PyData Miami 2019](https://www.youtube.com/watch?v=UrynTsa5LEE)
* [Spark로 알아보는 빅데이터 처리](https://www.slideshare.net/JoenggyuLenKim/spark-152302106)
* [Multi Source Data Analysis using Spark and Tellius : Meetup Video](http://blog.madhukaraphatak.com/multi-source-spark-tellius/)
  * [spark2.0-examples/src/main/scala/com/madhukaraphatak/examples/sparktwo/multisource](https://github.com/phatak-dev/spark2.0-examples/tree/2.4/src/main/scala/com/madhukaraphatak/examples/sparktwo/multisource)
  * [Multi Source Data Analysis using Spark and Tellius](https://www.slideshare.net/datamantra/multi-source-data-analysis-using-spark-and-tellius)
* [Spark 성능 최적화 및 튜닝 방법 - Part 1](https://nephtyws.github.io/data/spark-optimization-part-1/)
* [Master Spark fundamentals & optimizations](https://www.streamhub.co.uk/apache-spark-tuning-manual/)
* [Apache Spark Optimization Techniques | by Nabarun Chakraborti | Jun, 2020 | Medium](https://medium.com/@ch.nabarun/apache-spark-optimization-techniques-54864d4fdc0c)
* [ClickHouse Clustering for Spark Developer](http://blog.madhukaraphatak.com/clickouse-clustering-spark-developer/)
* [Data Modeling in Apache Spark - Part 1 : Date Dimension](http://blog.madhukaraphatak.com/data-modeling-spark-part-1/)
* [Data Modeling in Apache Spark - Part 2 : Working With Multiple Dates](http://blog.madhukaraphatak.com/data-modeling-spark-part-2/)
* [Concurrency in Spark](http://www.russellspitzer.com/2017/02/27/Concurrency-In-Spark/)
* [How we reduced our Apache Spark cluster cost using best practices](https://medium.com/iotransportation/how-we-reduced-our-apache-spark-cluster-cost-using-best-practices-ac1f176379ac)
* [Big Data file formats explained](https://medium.com/@luminousmen/big-data-file-formats-explained-73552c7ef4cd)
* [Why Spark on Ceph? (Part 1 of 3)](https://www.redhat.com/en/blog/why-spark-ceph-part-1-3)
* [Why Spark on Ceph? (Part 2 of 3)](https://www.redhat.com/en/blog/why-spark-ceph-part-2-3)
* [Why Spark on Ceph? (Part 3 of 3)](https://www.redhat.com/en/blog/why-spark-ceph-part-3-3)
* [Spark Delight — We’re building a better Apache Spark UI | by Jean Yves | Jun, 2020 | Towards Data Science](https://towardsdatascience.com/spark-delight-were-building-a-better-spark-ui-1b463840e243)
* [Overcoming Apache Spark’s biggest pain points | by Edson Hiroshi Aoki | Oct, 2020 | Towards Data Science](https://towardsdatascience.com/overcoming-apache-sparks-biggest-pain-points-b374cebcf6a4)
* [Speeding Time to Insight with a Modern ETL Approach - YouTube](https://www.youtube.com/watch?v=G5P8VbS0gXA) ETL -> ELT
* [Scale-Out Using Spark in Serverless Herd Mode! - YouTube](https://www.youtube.com/watch?v=zoFwtJ-j5co)
* [DBIOTransactionalCommit - Databricks](https://docs.databricks.com/_static/notebooks/dbio-transactional-commit.html)
* [입 개발 EMR에서는 sc.addFile, Databricks에서는 그냥 dbfs 폴더를 이용하자. | Charsyam's Blog](https://charsyam.wordpress.com/2021/03/25/%ec%9e%85-%ea%b0%9c%eb%b0%9c-emr%ec%97%90%ec%84%9c%eb%8a%94-sc-addfile-databricks%ec%97%90%ec%84%9c%eb%8a%94-%ea%b7%b8%eb%83%a5-dbfs-%ed%8f%b4%eb%8d%94%eb%a5%bc-%ec%9d%b4%ec%9a%a9%ed%95%98%ec%9e%90/)
* [Spark interview Q&As with coding examples in Scala - part 1 | Java-Success.com](https://www.java-success.com/spark-interview-qas-with-coding-examples-in-scala-part-1/)
* [How to Extract Deeper Value from Data in Legacy Applications with Analytics in a Cloud Data Lake - YouTube](https://www.youtube.com/watch?v=cylRHSsT24o)
* [Scala 3 and Spark?. After the release of Scala 3, one of… | by Filip Zybała | VirtusLab | Oct, 2021 | Medium](https://medium.com/virtuslab/scala-3-and-spark-389f7ecef71b)
* [Using Scala 3 with Spark | 47 Degrees](https://www.47deg.com/blog/using-scala-3-with-spark/)
* [Apache Spark #1 - 아키텍쳐 및 기본 개념](https://bcho.tistory.com/1387)
* [Practical Spark – Intro (1) – 1ambda](https://1ambda.blog/2021/12/20/practical-spark-1/)
* [Practical Spark – Tutorial (2) – 1ambda](https://1ambda.blog/2021/12/20/practical-spark-2/)
* [Practical Spark – Concept (3) – 1ambda](https://1ambda.blog/2021/12/20/practical-spark-3/)
* [Practical Spark – Architecture (4) – 1ambda](https://1ambda.blog/2021/12/21/practical-spark-4/)
* [Practical Spark – DataFrame (5) – 1ambda](https://1ambda.blog/2021/12/22/practical-spark-5/)
* [Practical Spark – Persistence (6) – 1ambda](https://1ambda.blog/2021/12/22/practical-spark-6/)
* [Practical Spark – Cache (7) – 1ambda](https://1ambda.blog/2021/12/25/practical-spark-cache-7/)
* [Practical Spark – SQL & Table (8) – 1ambda](https://1ambda.blog/2021/12/27/practical-spark-sql-table-8/)
* [Practical Spark – Join (9) – 1ambda](https://1ambda.blog/2021/12/27/practical-spark-9/)
* [Practical Spark – Memory (10) – 1ambda](https://1ambda.blog/2021/12/27/practical-spark-10/)
* [Practical Spark – Versions (11) – 1ambda](https://1ambda.blog/2021/12/28/practical-spark-11/)
* [Practical Spark – 자주 묻는 질문들 (12) – 1ambda](https://1ambda.blog/2022/01/02/practical-spark-12/)

# Apache Livy
* [Apache Livy](https://livy.incubator.apache.org/) A REST Service for Apache Spark
* [Apache Livy에서 Spark job stdout log를 보는 법 - Nephtyw’S Programming Stash](https://nephtyws.github.io/data/livy-logs-to-stdout/)

# API
* [Spark Programming Model : Resilient Distributed Dataset (RDD) - 2015](http://www.bogotobogo.com/Hadoop/BigData_hadoop_Apache_Spark_Programming_Model_RDD.php)
* [Apache Spark: Examples Of Transformations](https://www.supergloo.com/fieldnotes/apache-spark-examples-of-transformations/)
* [The RDD API By Example](http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html)
* [backtobazics.com/category/big-data/spark](http://backtobazics.com/category/big-data/spark/) example of API
* [APACHE SPARK: RDD, DATAFRAME OR DATASET?](http://www.agildata.com/apache-spark-rdd-vs-dataframe-vs-dataset/)
* [Learn Spark Scala with Clara: RDD](https://medium.com/@onsbouneb1/learn-spark-scala-with-clara-rdd-9395bd01cd52)
* [A modern guide to Spark RDDs](https://medium.com/@LeoLezcano/a-modern-guide-to-spark-rdds-725cd7c14059)
* [Anatomy of Apache Spark's RDD | LinkedIn](https://www.linkedin.com/pulse/anatomy-apache-sparks-rdd-deepak-rajak/)
* [Apache Spark’s Hidden REST API](http://arturmkrtchyan.com/apache-spark-hidden-rest-api)
* [Spark Session vs Spark Context](https://medium.com/@achilleus/spark-session-10d0d66d1d24)
* Exploring Spark DataSource V2
  * [Part 1 : Limitations of Data Source V1 API](http://blog.madhukaraphatak.com/spark-datasource-v2-part-1/)
  * [Part 2 : Anatomy of V2 Read API](http://blog.madhukaraphatak.com/spark-datasource-v2-part-2/)
  * [Part 3 : In-Memory DataSource](http://blog.madhukaraphatak.com/spark-datasource-v2-part-3/)
  * [Part 4 : In-Memory DataSource with Partitioning](http://blog.madhukaraphatak.com/spark-datasource-v2-part-4/)
  * [Part 5 : Filter Push](http://blog.madhukaraphatak.com/spark-datasource-v2-part-5/)
  * [Part 6 : Anatomy of V2 Write API](http://blog.madhukaraphatak.com/spark-datasource-v2-part-6/)
  * [Part 7 : Meetup Talk](http://blog.madhukaraphatak.com/spark-datasource-v2-part-7/)
* [Migrating to Spark 2.4 Data Source API](http://blog.madhukaraphatak.com/migrate-spark-datasource-2.4/)
* [Map Filter Reduce & Lambda in Python & Scala| Comparison of Lambda Syntaxes Step by Step| Beginners - YouTube](https://www.youtube.com/watch?v=ugbad1B0IZ4)
* aggregate

  ```
  scala> val rdd = sc.parallelize(List(1, 2, 3, 3))
  rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:21

  scala> rdd.aggregate((0, 0))((x, y) => (x._1 + y, x._2 - y), (x, y) => (x._1 + y._1, x._2 + y._2))
  res10: (Int, Int) = (9,-9)

  scala> rdd.map(t => (t, -t)).reduce((a, b) => (a._1 + b._1, a._2 + b._2))
  res11: (Int, Int) = (9,-9)
  ```
* aggregateByKey
  * [AggregateByKey implements Collect_list in Spark 1.4](http://alvincjin.blogspot.kr/2015/09/aggregatebykey-implements-collectlist.html)
* Array [Deep Dive into Apache Spark Array Functions | by Neeraj Bhadani | Expedia Group Technology | Medium](https://medium.com/expedia-group-tech/deep-dive-into-apache-spark-array-functions-720b8fbfa729)
* combineByKey
  * [Using combineByKey in Apache-Spark](http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/)
  * [Spark PairRDDFunctions: CombineByKey](http://codingjunkie.net/spark-combine-by-key/)
  * [Apache Spark combineByKey Explained](http://www.edureka.co/blog/apache-spark-combinebykey-explained)
* DataFrames
  * [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)
  * [spark2.0 dataframe의 filter,where,isin,select,contains,col,between,withColumn, 예제](http://knight76.tistory.com/entry/spark20-dataframe%EC%9D%98-filterwhereisinselect)
  * [Spark: Connecting to a jdbc data-source using dataframes](http://www.infoobjects.com/spark-connecting-to-a-jdbc-data-source-using-dataframes/)
  * [입 개발 Spark 에서 Database 빨리 덤프하는 법(Parallelism) | Charsyam's Blog](https://charsyam.wordpress.com/2021/09/06/%EC%9E%85-%EA%B0%9C%EB%B0%9C-spark-%EC%97%90%EC%84%9C-database-%EB%B9%A8%EB%A6%AC-%EB%8D%A4%ED%94%84%ED%95%98%EB%8A%94-%EB%B2%95parallelism/) Spark JDBC
  * [where과 filter의 차이](http://knight76.tistory.com/entry/spark-where%EA%B3%BC-filter%EC%9D%98-%EC%B0%A8%EC%9D%B4)
  * [Using spark data frame for sql](https://www.slideshare.net/charsyam2/using-spark-data-frame-for-sql)
  * [Selecting Dynamic Columns In Spark DataFrames (aka Excluding Columns)](http://bailiwick.io/2017/08/08/selecting-dynamic-columns-in-spark-dataframes/)
  * [Spark: Elegantly Aggregate DataFrame by One Key Column](https://medium.com/@kieran_77908/spark-elegantly-aggregate-dataframe-by-one-key-column-6dbbcf546e1d)
  * [A practical introduction to Spark’s Column- part 1](https://medium.com/@achilleus/https-medium-com-achilleus-a-practical-introduction-to-sparks-column-3f5fe83125c9)
  * [A practical introduction to Spark’s Column- part 2](https://medium.com/@achilleus/a-practical-introduction-to-sparks-column-part-2-1e52f1d29eb1)
  * [Different approaches to manually create Spark DataFrames](https://medium.com/@mrpowers/manually-creating-spark-dataframes-b14dae906393)
  * [Sending Spark DataFrame via mail](https://medium.com/@n.suthar.online/sending-spark-dataframe-via-mail-f396b1810d89)
  * [How I achieved 3x speedup for joins over Spark dataframes](https://medium.com/@cbhaavan/3x-performance-improvement-for-joins-on-spark-dataframe-d79e93a5fc48)
  * [Deep dive into Apache Spark Window Functions | by Neeraj Bhadani | Expedia Group Technology | Medium](https://medium.com/expedia-group-tech/deep-dive-into-apache-spark-window-functions-7b4e39ad3c86)
  * [Making the Spark DataFrame composition type safe(r) | by Iaroslav Zeigerman | Feb, 2021 | Medium](https://izeigerman.medium.com/making-the-spark-dataframe-composition-type-safe-r-7b6fed524ec2)
  * [How to add row numbers to a Spark DataFrame? | Data Programmers](https://www.datarchy.tech/code/20200820_spark_rows_number/)
* Datasets
  * [Introducing Spark Datasets](https://databricks.com/blog/2016/01/04/introducing-spark-datasets.html)
  * [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)
  * [RDDs, DataFrames and Datasets in Apache Spark - NE Scala 2016](https://www.youtube.com/watch?v=pZQsDloGB4w&t=740s)
  * Spark2.0 New Features
    * [(1) DataSet](http://www.popit.kr/spark2-0-new-features1-dataset/)
    * [(2) Structured Streaming – 1편](https://www.popit.kr/spark2-0-new-features2-structured-streaming-1%ED%8E%B8/)
  * [Transforming Spark Datasets using Scala transformation functions](http://olivermascarenhas.com/spark/2019/09/25/how-to-transform-a-spark-dataset.html)
  * [(2) Solution to Spark Auto Schema inference (String) for JSON Array / JSON Object/Record/Row Problem | LinkedIn](https://www.linkedin.com/pulse/spark-auto-schema-inference-json-objects-array-shubham-chakraborty/)
* DateTime [Deep Dive into Apache Spark DateTime Functions](https://medium.com/expedia-group-tech/deep-dive-into-apache-spark-datetime-functions-b66de737950a)
* distinct
  * [~동일성, 동등성, Spark의 distinct~](https://leeyh0216.github.io/dev/lang/spark/2017/04/30/dev-spark-equals.html)
* groupByKey
  * [Avoid GroupByKey](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html)
* HashPartitioner
  * [Apache Spark - HashPartitioner : How does it work?](http://stackoverflow.com/questions/31424396/apache-spark-hashpartitioner-how-does-it-work)
  * [Partition by Hash on Keys](https://bzhangusc.wordpress.com/2014/06/17/partition-by-hash-on-keys/)
* join
  * [RDD join 예제](http://knight76.tistory.com/entry/spark-RDD-join-%EC%98%88%EC%A0%9C)
  * [join 예제](http://knight76.tistory.com/entry/spark-join-%EC%98%88%EC%A0%9C)
  * [Joins in Apache Spark — Part 1](https://medium.com/@akhilanand.bv/https-medium-com-joins-in-apache-spark-part-1-dabbf3475690)
  * [Joins in Apache Spark — Part 2](https://medium.com/@akhilanand.bv/https-medium-com-joins-in-apache-spark-part-2-5b038bc7455b)
  * [Joins in Apache Spark — Part 3](https://medium.com/@achilleus/https-medium-com-joins-in-apache-spark-part-3-1d40c1e51e1c)
* persist
  * [RDD persist() or cache() 시 주의사항](http://tomining.tistory.com/84)
* SQL
  * [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)
    * [Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column)
    * [Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
    * [Row](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row)
  * [spark-csv - CSV Data Source for Apache Spark 1.x](https://github.com/databricks/spark-csv/)
    * [TextFileSuite.scala](https://github.com/databricks/spark-csv/blob/master/src/test/scala/com/databricks/spark/csv/util/TextFileSuite.scala)
  * [Spark SQL CSV Examples](https://www.supergloo.com/fieldnotes/spark-sql-csv-examples/)
  * [github.com/yhuai/spark/tree/eb77ee39b8616cb367541503baf7c07695ef1ec0/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv](https://github.com/yhuai/spark/tree/eb77ee39b8616cb367541503baf7c07695ef1ec0/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv)
  * [Dataframes from CSV files in Spark 1.5: automatic schema extraction, neat summary statistics, & elementary data exploration](http://www.nodalpoint.com/spark-dataframes-from-csv-files/)
  * [Spark 2.0 read csv number of partitions (PySpark)](http://stackoverflow.com/questions/38128233/spark-2-0-read-csv-number-of-partitions-pyspark)
  * [How to read csv file as DataFrame?](http://stackoverflow.com/questions/29704333/how-to-read-csv-file-as-dataframe)
  * [How to change column types in Spark SQL's DataFrame?](http://stackoverflow.com/questions/29383107/how-to-change-column-types-in-spark-sqls-dataframe)
  * [Working with Nested Data Using Higher Order Functions in SQL on Databricks](https://databricks.com/blog/2017/05/24/working-with-nested-data-using-higher-order-functions-in-sql-on-databricks.html)
    * Hadoop과 Spark은 nested structs, array, map 등과 같은 복잡하고 다양한 데이터를 처리하는 훌륭한 도구이지만 SQL에서 사용하는 건 어려움
    * Databricks 3.0에 추가된 TRANSFORM 연산과 Spark SQL에 추가된 "Higher Order Functions"를 소개(SPARK-19480)
  * [Spark SQL under the hood – part I](http://virtuslab.com/blog/spark-sql-hood-part-i/)
  * [Five Spark SQL Utility Functions to Extract and Explore Complex Data Types - Tutorial on how to do ETL on data from Nest and IoT Devices](https://databricks.com/blog/2017/06/13/five-spark-sql-utility-functions-extract-explore-complex-data-types.html)
  * [Querying our Data Lake in S3 using Zeppelin and Spark SQL](https://medium.com/@deniseschlesinger/querying-our-data-lake-in-s3-using-zeppelin-and-spark-sql-be50c3b7a613)
  * [Learning Spark SQL with Zeppelin](https://hortonworks.com/tutorial/learning-spark-sql-with-zeppelin/)
  * [SQL Pivot: Converting Rows to Columns](https://databricks.com/blog/2018/11/01/sql-pivot-converting-rows-to-columns.html) 2.4
  * [SQL at Scale with Apache Spark SQL and DataFrames — Concepts, Architecture and Examples](https://towardsdatascience.com/sql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f)
  * [A Deep Dive into Query Execution Engine of Spark SQL - Maryann Xue](https://www.youtube.com/watch?v=ywPuZ_WrHT0)
  * [A Deep Dive into Spark SQL's Catalyst Optimizer - Yin Huai](https://www.youtube.com/watch?v=RmUn5vHlevc)
* trigger [Spark Trigger Options](https://dzone.com/articles/spark-trigger-options)

# Book
* [더북(TheBook): 스파크를 다루는 기술](https://thebook.io/006908/) 4~6장만
* [**Mastering Apache Spark 2.0**](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/)
* [Advanced Analytics with Spark Source Code](https://github.com/sryza/aas)
* [Best Apache Spark and Scala Books for Mastering Spark Scala](https://data-flair.training/blogs/best-apache-spark-scala-books/)
* [Spark for Data Analyst](https://wikidocs.net/book/1686) Spark SQL

# Conference
* [Spark Day 2017@Seoul(Spark Bootcamp)](https://www.slideshare.net/SangbaeLim/spark-bootcamp2017inseoul-finalpt20170626distv1)
* [Spark Day 2017- Spark 의 과거, 현재, 미래](https://www.slideshare.net/MoonSooLee2/spark-day-2017-spark)
* [Spark & Zeppelin을 활용한 한국어 텍스트 분류](https://www.slideshare.net/JunKim22/spark-zeppelin-77273056)
  * [Spark & Zeppelin을 활용한 한국어 텍스트 분류](https://www.facebook.com/groups/sparkkoreauser/permalink/1465550026840229/)
* [Zeppelin 노트북: NSMC Word2Vec & Sentiment Classification](https://github.com/uosdmlab/nsmc-zeppelin-notebook)
* [Spark, Mesos, Zeppelin, HDFS를 활용한 대용량 보안 데이터 분석](https://engineering.linecorp.com/ko/blog/detail/60)
* [2020 데이터 컨퍼런스 "Spark+Cassandra 기반 빅데이터를 활용한 추천시스템 서빙 파이프라인 최적화" / 박수성 SSG.COM 파트너 - YouTube](https://www.youtube.com/watch?v=CQD-85a-NRs)
* [Tale of Scaling Zeus to Petabytes of Shuffle Data @Uber - YouTube](https://www.youtube.com/watch?v=8n8zDvv59_A)
* [Sub-Second Analytics for User-Facing Applications with Apache Spark and Rockset - YouTube](https://www.youtube.com/watch?v=-gRFwzK5L2s)

# Deep Learning
* [yahoo/CaffeOnSpark](https://github.com/yahoo/CaffeOnSpark)
* [CaffeOnSpark Open Sourced for Distributed Deep Learning on Big Data Clusters](http://yahoohadoop.tumblr.com/post/139916563586/caffeonspark-open-sourced-for-distributed-deep)
* [Large Scale Distributed Deep Learning on Hadoop Clusters](http://yahoohadoop.tumblr.com/post/129872361846/large-scale-distributed-deep-learning-on-hadoop)
* [SparkNet: Training Deep Networks in Spark](http://arxiv.org/abs/1511.06051)
  * [Spark + Deep Learning: Distributed Deep Neural Network Training with SparkNet](http://www.kdnuggets.com/2015/12/spark-deep-learning-training-with-sparknet.html)
* [large scale deep-learning_on_spark](http://www.slideshare.net/deview/246-large-scale-deeplearningonspark)
* [DeepSpark: Spark-Based Deep Learning Supporting Asynchronous Updates and Caffe Compatibility](http://hgpu.org/?p=15511)
* [The Unreasonable Effectiveness of Deep Learning on Spark](https://databricks.com/blog/2016/04/01/unreasonable-effectiveness-of-deep-learning-on-spark.html)
* [GPU Acceleration in Databricks Speeding Up Deep Learning on Apache Spark](https://databricks.com/blog/2016/10/27/gpu-acceleration-in-databricks.html)
* [Deep Learning on Databricks - Integrating with TensorFlow, Caffe, MXNet, and Theano](https://databricks.com/blog/2016/12/21/deep-learning-on-databricks.html)
* Deep Learning With Apache Spark
  * [Part 1](https://towardsdatascience.com/deep-learning-with-apache-spark-part-1-6d397c16abd)
  * [Part 2](https://towardsdatascience.com/deep-learning-with-apache-spark-part-2-2a2938a36d35)
* [Deep Learning Pipelines for Apache Spark](https://github.com/databricks/spark-deep-learning)

# Docker
* practice
  * [practice - install zeppelin docker image, read and adjust json files](https://github.com/hyunjun/practice_private/tree/master/interview_bonial_assignment)
* [DIT4C image for Apache Zeppelin](https://hub.docker.com/r/dit4c/dit4c-container-zeppelin/)
* [hub.docker.com/r/k3vin/polynote-spark](https://hub.docker.com/r/k3vin/polynote-spark)
* [spark-scala-tutorial A free tutorial for Apache Spark](https://github.com/deanwampler/spark-scala-tutorial) docker jupyter notebook
* [Apache Spark on Docker](https://github.com/sequenceiq/docker-spark)
* Distributed Pricing Engine using Dockerized Spark on YARN w/ HDP 3.0
  * [Part 1/4](https://hortonworks.com/blog/distributed-pricing-engine-using-dockerized-spark-yarn-w-hdp-3-0-part-1-4/)
  * [Part 2/4](https://hortonworks.com/blog/distributed-pricing-engine-using-dockerized-spark-yarn-w-hdp-3-0-part-2-4/)
  * [Part 3/4](https://hortonworks.com/blog/distributed-pricing-engine-using-dockerized-spark-yarn-w-hdp-3-0-part-3-4/)
  * [Part 4/4](https://hortonworks.com/blog/distributed-pricing-engine-using-dockerized-spark-yarn-w-hdp-3-0-part-4-4/)
* [Getting Started with PySpark for Big Data Analytics, using Jupyter Notebooks and Docker](https://medium.com/@GaryStafford/getting-started-with-pyspark-for-big-data-analytics-using-jupyter-notebooks-and-docker-ba39d2e3d6c7)
* [DIY: Apache Spark & Docker. Set up a Spark cluster in Docker from… | by Shane De Silva | Towards Data Science](https://towardsdatascience.com/diy-apache-spark-docker-bb4f11c10d24)

# GraphX
* [GraphX](https://spark.apache.org/docs/1.0.0/graphx-programming-guide.html)
* [Spark Streaming and GraphX at Netflix - Apache Spark Meetup, May 19, 2015](https://www.youtube.com/watch?v=gqgPtcDmLGs)
* [스사모 테크톡 - GraphX](http://www.slideshare.net/sangwookimme/graphx)
* [Computing Shortest Distances Incrementally with Spark](http://insightdataengineering.com/blog/incr-short-dist-graphx/)
* [Strata 2016 - This repo is for MLlib/GraphX tutorial in Strata 2016](https://github.com/jayantshekhar/strata-2016)
* [Processing Hierarchical Data using Spark Graphx Pregel API](http://www.qubole.com/blog/processing-hierarchical-data-using-spark-graphx-pregel-api/)
  * GraphX API를 사용하는 예제와 방법
* [Community detection in graph](https://gitlab.com/siddhinath/community) Girvan newman algorithm

# Hbase
* example
  * [HBaseTest.scala, hbase_inputformat.py](https://gist.github.com/hyunjun/d9d73c5fe8a7f7b17b28)
* [I simple API to interact with HBase with Spark](https://github.com/tmalaska/SparkOnHBase)
* [Apache Spark Comes to Apache HBase with HBase-Spark Module](http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/?elq=b8eb31d395f14250a2c264604a98ed0e&elqCampaignId=987&elqaid=2217&elqat=1&elqTrackId=8472a26fbfcb4511b1a86953234a7bed)
* [HBase Integration with Spark | How to Integrate HBase with Spark | Spark Integration with HBase](https://www.youtube.com/watch?v=gGwB0kCcdu0)
* [How to create Spark Dataframe on HBase table](https://medium.com/@thomaspt748/how-to-create-spark-dataframe-on-hbase-table-e9c8db31bb30)

# Ignite
* [Ignite](https://ignite.apache.org/features/igniterdd.html) - Spark Shared RDDs

# Installation
* [Installing Apache Spark 2.3.0 on macOS High Sierra](https://medium.com/luckspark/installing-spark-2-3-0-on-macos-high-sierra-276a127b8b85)
* [How to install and run Spark 2.0 on HDP 2.5 Sandbox](https://community.hortonworks.com/articles/53029/how-to-install-and-run-spark-20-on-hdp-25-sandbox.html)
* [Apache Spark installation on Windows 10](https://hernandezpaul.wordpress.com/2016/01/24/apache-spark-installation-on-windows-10/)
* [Spark StandAlone 설치부터 예제 테스트까지](http://hellowuniverse.com/2017/03/08/spark-standalone-%EC%84%A4%EC%B9%98%EB%B6%80%ED%84%B0-%EC%98%88%EC%A0%9C-%ED%85%8C%EC%8A%A4%ED%8A%B8%EA%B9%8C%EC%A7%80/)
* [Hadoop, Spark 설치](https://github.com/likejazz/likejazz.github.io/wiki/Hadoop,-Spark)
* [Spark (scala) 개발환경 설정 (window)](https://blog.naver.com/haiteam/221208062178)
* [How to Install Scala and Apache Spark on MacOS](https://medium.freecodecamp.org/installing-scala-and-apache-spark-on-mac-os-837ae57d283f)
* [Apache Spark setup with Gradle, Scala and IntelliJ](https://medium.com/@faizanahemad/apache-spark-setup-with-gradle-scala-and-intellij-2eeb9f30c02a)
* [Create Spark Scala SBT project in Intellij Idea. 1-minute tutorial - YouTube](https://www.youtube.com/watch?v=XLgzBqrEZ0U)
* [pocketcluster - One-Step Spark/Hadoop Installer v0.1.0](https://github.com/stkim1/pocketcluster)
* [Spark 2: How to install it on Windows in 5 steps](https://medium.com/@dvainrub/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3)
* [Apache Spark Setup in Windows|Intellij IDE|CommandLine|Databricks|Zeppelin|All Methods Covered 2021. - YouTube](https://www.youtube.com/watch?v=S5p-2vlUBYo)

# Kubernetes
* [Introduction to Spark on Kubernetes](https://banzaicloud.github.io/blog/spark-k8s/)
* [What’s New for Apache Spark on Kubernetes in the Upcoming Apache Spark 2.4 Release](https://databricks.com/blog/2018/09/26/whats-new-for-apache-spark-on-kubernetes-in-the-upcoming-apache-spark-2-4-release.html)
  * 2.4 preview. Kubernetes 지원 강화, PySpark/Spark R 지원 추가 등
* [Spark day 2017@Seoul - Spark on Kubernetes](https://www.slideshare.net/jerryjung7/spark-day-2017seoul)
* Scalable Spark Deployment using Kubernetes
  * [Docker Image and Kubernetes Configurations for Spark 2.x](https://github.com/phatak-dev/kubernetes-spark)
  * [Part 1 : Introduction to Kubernetes](http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-1/)
  * [Part 2 : Installing Kubernetes Locally using Minikube](http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-2/)
  * [Part 3 : Kubernetes Abstractions](http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-3/)
  * [Part 4 : Service Abstractions](http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-4/)
  * [Part 5 : Building Spark 2.0 Docker Image](http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-5/)
  * [Part 6 : Building Spark 2.0 Two Node Cluster](http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-6/)
  * [Part 7 : Dynamic Scaling and Namespaces](http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-7/)
* Auto Scaling Spark in Kubernetes
  * [Part 1 : Introduction](http://blog.madhukaraphatak.com/horizontal-scaling-k8s-part-1/)
  * [Part 2 : Spark Cluster Setup](http://blog.madhukaraphatak.com/horizontal-scaling-k8s-part-2/)
  * [Part 3 : Scaling Spark Workers](http://blog.madhukaraphatak.com/horizontal-scaling-k8s-part-3/)
* [The anatomy of Spark applications on Kubernetes](https://banzaicloud.com/blog/spark-k8s-internals/)
  * Kubernetes에 대한 Spark의 실험적 지원과 인-클러스터 클라이언트 모드에 대한 향후 지원에 대해 설명
  * Spark driver, Executor, Executor Shuffle Service, Resource Staging Server
* [How to build Spark from source and deploy it to a Kubernetes cluster in 60 minutes](https://towardsdatascience.com/how-to-build-spark-from-source-and-deploy-it-to-a-kubernetes-cluster-in-60-minutes-225829b744f9)
* [Apache Spark workloads on Kubernetes](https://spot.io/blog/apache-spark-workloads-on-kubernetes/)
* [Apache Spark Streaming in K8s with ArgoCD & Spark Operator - YouTube](https://www.youtube.com/watch?v=9o50GiPPjpE)
* [Spark on Kubernetes - Gang Scheduling with YuniKorn - Cloudera Blog](https://blog.cloudera.com/spark-on-kubernetes-gang-scheduling-with-yunikorn/)
* [Superworkflow of Graph Neural Networks with K8S and Fugue - YouTube](https://www.youtube.com/watch?v=-aEZjQiqSFA) word2vec node2vec

# Library
* [Hadoop Tutorial: the new beta Notebook app for Spark & SQL](https://vimeo.com/125792752)
* [AWS Athena Data Source for Apache Spark](https://github.com/tmheo/spark-athena)
* [BigDL: Distributed Deep learning on Apache Spark](https://software.intel.com/en-us/articles/bigdl-distributed-deep-learning-on-apache-spark)
  * [BigDL: Distributed Deep learning on Apache Spark](https://github.com/intel-analytics/BigDL)
* [CLOUD DATAPROC - Google Cloud Dataproc is a managed Spark and Hadoop service that is fast, easy to use, and low cost](https://cloud.google.com/dataproc/)
  * [구글, 스파크·하둡 관리 클라우드 서비스 공개](http://www.bloter.net/archives/239483)
  * [Google Cloud Dataproc 사용하기(http://whitechoi.tistory.com/48)
* [couchbase-spark-connector - The Official Couchbase Spark Connector](https://github.com/couchbase/couchbase-spark-connector)
* [CueSheet - a framework for writing Apache Spark 2.x applications more conveniently](https://github.com/kakao/cuesheet)
  * [No More "sbt assembly": Rethinking Spark-Submit using CueSheet](http://www.slideshare.net/jongwookkim/rethinking-sparksubmit-using-cuesheet)
* [Delta Lake - Reliable Data Lakes at Scale](https://delta.io/)
  * [Delta Lake on Databricks - Databricks](https://databricks.com/product/delta-lake-on-databricks)
  * [Tutorial: How Delta Lake Supercharges Data Lakes - YouTube](https://www.youtube.com/watch?v=u1VfOiHVeMI)
  * [SmartSQL Queries powered by Delta Engine on Lakehouse - YouTube](https://www.youtube.com/watch?v=PCVyk8npl-k)
  * [Making Apache Spark™ Better with Delta Lake - YouTube](https://www.youtube.com/watch?v=LJtShrQqYZY)
  * [Tech Talk: Top Tuning Tips for Spark 3.0 and Delta Lake on Databricks - YouTube](https://www.youtube.com/watch?v=hcoMHnTcvmg)
  * [Delta Lakehouse Data Profiler and SQL Analytics Demo - YouTube](https://www.youtube.com/watch?v=58nT52VTzsQ)
  * [Optimising Geospatial Queries with Dynamic File Pruning - YouTube](https://www.youtube.com/watch?v=3D5WhCqfOo8)
  * [Demystifying Delta Lake. Data Brew | Episode 3 - YouTube](https://www.youtube.com/watch?v=Zws7u9DC5SE)
  * [Delta Lake on Databricks Demo - YouTube](https://www.youtube.com/watch?v=BMO90DI82Dc)
  * [Make Reliable ETL Easy on Delta Lake - YouTube](https://www.youtube.com/watch?v=1AotjGNLo3Q)
  * [Building Lakehouses on Delta Lake with SQL Analytics Primer - YouTube](https://www.youtube.com/watch?v=n9cRw6AkNDQ)
  * [Massive Data Processing in Adobe Experience Platform Using DeltaLake | by Jaemi Bremner | Adobe Tech Blog | Medium](https://medium.com/adobetech/massive-data-processing-in-adobe-experience-platform-using-deltalake-de5ceef2c150)
  * [Multi-Table Transactions with LakeFS and Delta Lake - YouTube](https://www.youtube.com/watch?v=OYhbwrBCM9I)
* [Dr. Elephant Self-Serve Performance Tuning for Hadoop and Spark](https://engineering.linkedin.com/blog/2016/04/dr-elephant-open-source-self-serve-performance-tuning-hadoop-spark)
* EMR
  * [Large-Scale Machine Learning with Spark on Amazon EMR](http://blogs.aws.amazon.com/bigdata/post/Tx21LOP0UQ2ZA9N/Large-Scale-Machine-Learning-with-Spark-on-Amazon-EMR)
  * [Amazon EMR, Apache Spark 지원 시작](https://aws.amazon.com/ko/blogs/korea/new-apache-spark-on-amazon-emr)
  * [Spark on EMR](https://github.com/awslabs/emr-bootstrap-actions/tree/master/spark)
  * [**(BDT309) Data Science & Best Practices for Apache Spark on Amazon EMR**](http://www.slideshare.net/AmazonWebServices/bdt309-data-science-best-practices-for-apache-spark-on-amazon-emr)
  * [Starburst’s Presto on AWS up to 18x faster than EMR](https://www.starburstdata.com/technical-blog/starburst-presto-on-aws-18x-faster-than-emr/) Presto의 엔터프라이즈 빌드를 제공하는 Starbust에서 AWS와 EMR 환경에서 벤치마크한 결과 소개
  * [Optimize Spark jobs on EMR Cluster](https://medium.com/@sharad_21643/https-medium-com-optimize-spark-jobs-on-emr-cluster-6b1a97711adb)
* [Envelope - a configuration-driven framework for Apache Spark that makes it easy to develop Spark-based data processing pipelines on a Cloudera EDH](https://github.com/cloudera-labs/envelope/)
  * Envelope과 함께 Apache Spark, Apache Kudu 및 Apache Impala를 사용하여 Cloudera enterprise data hub (EDH)에 구현하는 방법
  * [Configuration specification](https://github.com/cloudera-labs/envelope/blob/master/docs/configurations.adoc#planners)
  * [Bi-temporal data modeling with Envelope](http://blog.cloudera.com/blog/2017/05/bi-temporal-data-modeling-with-envelope/)
  * [Cloudera Enterprise Data Hub - Our flagship can now be yours](https://www.cloudera.com/products/enterprise-data-hub.html)
* [flambo - A Clojure DSL for Apache Spark](https://github.com/yieldbot/flambo)
* [GraphFrames: DataFrame-based Graphs](https://github.com/graphframes/graphframes)
  * [On-Time Flight Performance with GraphFrames for Apache Spark](https://databricks.com/blog/2016/03/16/on-time-flight-performance-with-spark-graphframes.html)
  * [An introduction to Spark GraphFrame with examples analyzing the Wikipedia link graph](https://towardsdatascience.com/an-introduction-to-spark-graphframe-with-examples-analyzing-the-wikipedia-link-graph-67e58c20a107)
* [Hail: Scalable Genomics Analysis with Apache Spark](http://blog.cloudera.com/blog/2017/05/hail-scalable-genomics-analysis-with-spark/)
  * Apache Spark로 유전체 분석을 수행하는 도구 인 Hail에 대한 개요
  * 샘플의 품질을 계산하고 간단한 게놈 차원의 연관 연구를 수행하는 예제 실행으로 시연하는 간단하고 강력한 프로그래밍 모델을 보유
* [Hudi - Spark Library for Hadoop Upserts And Incrementals https://uber.github.io/hudi ](https://github.com/uber/hudi)
  * [The Evolution of Uber’s 100+ Petabyte Big Data Platform](https://www.infoq.com/news/2018/11/uber-big-data-evolution)
  * [Marmaray: An Open Source Generic Data Ingestion and Dispersal Framework and Library for Apache Hadoop](https://eng.uber.com/marmaray-hadoop-ingestion-open-source/)
  * [Peloton: Uber’s Unified Resource Scheduler for Diverse Cluster Workloads](https://eng.uber.com/peloton)
  * [Building an analytical data lake with Apache Spark and Apache Hudi - Part 1](https://olivermascarenhas.com/2020-04-13-building-analytical-datalake-with-apache-spark-and-apache-hudi/)
* Hydrogen
  * [Project Hydrogen: State of the Art Deep Learning on Apache Spark - Singapore Spark+AI Meetup](https://www.youtube.com/watch?v=lqACnjdhtgA)
* [Infinispan Spark connector 0.1 released!](http://blog.infinispan.org/2015/08/infinispan-spark-connector-01-released.html)
  * [infinispan-spark](https://github.com/infinispan/infinispan-spark)
  * [infinispan-spark-connector-examples](https://github.com/tedwon/infinispan-spark-connector-examples)
* [IMLLIB - Factorization Machines (LibFM) Field-Awared Factorization Machine (FFM) Conditional Random Fields (CRF) Adaptive learning rate optimizer (AdaGrad, Adam)](https://github.com/Intel-bigdata/imllib-spark)
* [Lighthouse - a library for data lakes built on top of Apache Spark](https://github.com/datamindedbe/lighthouse)
  * [Organize your data lake using Lighthouse](https://medium.com/datamindedbe/organize-your-data-lake-using-lighthouse-a8bcf9c59594)
* [Livy, the Open Source REST Service for Apache Spark, Joins Cloudera Labs](http://blog.cloudera.com/blog/2016/07/livy-the-open-source-rest-service-for-apache-spark-joins-cloudera-labs/)
  * [Livy: A REST Web Service For Apache Spark](http://www.slideshare.net/JenAman/livy-a-rest-web-service-for-apache-spark)
* [MapR-DB Spark Connector with Secondary Indexes](https://hackernoon.com/mapr-db-spark-connector-with-secondary-indexes-df41909f28ea)
  * [MapR-DB Spark Connector Performance Tests](https://medium.com/@anicolaspp/mapr-db-spark-connector-performance-tests-2ec6e788e867)
  * [Extending MapR Database Queries Using Scala Polymorphic Types](https://hackernoon.com/extending-mapr-database-queries-using-scala-polymorphic-types-ee7a1c7ac2a7)
* [native_spark - new arguably faster implementation of Apache Spark from scratch in Rust](https://github.com/rajasekarv/native_spark)
  * [Spark implemented in Rust with promising results](https://www.reddit.com/r/rust/comments/dltdmm/spark_implemented_in_rust_with_promising_results/)
* [snappydata - Unified Online Transactions + Analytics + Probabilistic Data Platform](http://www.snappydata.io/blog/snappydata-technical-vision)
  * [SnappyData: OLTP + OLAP Database built on Apache Spark http://www.snappydata.io](https://github.com/SnappyDataInc/snappydata)
* [spark-annoy: Building Annoy Index on Apache Spark](https://github.com/mskimm/spark-annoy)
* [spark cassandra connector - 스파크에 카산드라 연동하는 라이브러리](http://knight76.tistory.com/entry/spark-spark-cassandra-connector-%EC%8A%A4%ED%8C%8C%ED%81%AC%EC%97%90-%EC%B9%B4%EC%82%B0%EB%93%9C%EB%9D%BC-%EC%97%B0%EB%8F%99%ED%95%98%EB%8A%94-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC)
* [spark-fatJAR-example: scala-spark build fat-jar example](https://github.com/Moons08/spark-fatJAR-example)
  * [spark-submit을 위한 스파크 앱 JAR 생성하기 - Mk’s Blog](https://moons08.github.io/programming/scala_build/)
* [spark-indexed - An efficient updatable key-value store for Apache Spark](https://github.com/amplab/spark-indexedrdd)
* [Sparkline SNAP](http://sparklinedata.com/)
  * [Introducing Sparkline SNAP: An Integrated OLAP platform on Spark](http://sparklinedata.com/sparkline-snap-olap-on-spark/)
* [spark-nkp Natural Korean Processor for Apache Spark](https://github.com/uosdmlab/spark-nkp)
* [Spark Notebook](http://spark-notebook.io/)
* [SparMysqlSample](https://github.com/hoonmokmoon/SparMysqlSample)
* [spark-nlp - Natural Language Understanding Library for Apache Spark](https://github.com/JohnSnowLabs/spark-nlp)
  * [Spark NLP: Getting Started With The World’s Most Widely Used NLP Library In The Enterprise](https://www.kdnuggets.com/2019/06/spark-nlp-getting-started-with-worlds-most-widely-used-nlp-library-enterprise.html)
  * [Spark NLP 101: Document Assembler](https://medium.com/spark-nlp/spark-nlp-101-document-assembler-500018f5f6b5)
  * [Spark NLP: Installation on Mac and Linux (Part-II)](https://medium.com/spark-nlp/introduction-to-spark-nlp-installation-and-getting-started-part-ii-d009f7a177f3)
  * [Introduction to Spark NLP: Foundations and Basic Components](https://towardsdatascience.com/introduction-to-spark-nlp-foundations-and-basic-components-part-i-c83b7629ed59)
  * [Spark NLP 101: LightPipeline](https://medium.com/spark-nlp/spark-nlp-101-lightpipeline-a544e93f20f1)
  * [Spark in Docker in Kubernetes: A Practical Approach for Scalable NLP | by Jürgen Schmidl | Towards Data Science](https://towardsdatascience.com/spark-in-docker-in-kubernetes-a-practical-approach-for-scalable-nlp-9dd6ef47c31e)
* [spark-packages - A community index of packages for Apache Spark](http://spark-packages.org/)
  * [스칼라 의존성, 패키지 검색하는 웹 - http://spark-packages.org](http://knight76.tistory.com/entry/scala-%EC%8A%A4%EC%B9%BC%EB%9D%BC-%EC%9D%98%EC%A1%B4%EC%84%B1-%ED%8C%A8%ED%82%A4%EC%A7%80-%EA%B2%80%EC%83%89%ED%95%98%EB%8A%94-%EC%9B%B9-httpsparkpackagesorg)
* [spark-ts - Time Series for Spark (The spark-ts Package)](https://github.com/sryza/spark-timeseries)
* [spark-xml - XML data source for Spark SQL and DataFrames](https://github.com/databricks/spark-xml)
* [StreamSets Transformer - an execution engine within the StreamSets DataOps platform that allows any user to create data processing pipelines that execute on Spark](https://streamsets.com/products/transformer/)
  * [Custom Scala Project for StreamSets Transformer](https://medium.com/@iamontheinet/iamontheinet-streamsets-11cf8ba480a)
* zio
  * [SF Scala: Enhancing Spark's Power with ZIO, Qubism and NLP at Scale, Using Nix for Haskell](https://www.youtube.com/watch?v=Ov7WZroBkv0)
  * [Accelerating Spark with ZIO by Leo Benkel - YouTube](https://www.youtube.com/watch?v=bWgVGzb5-H8)

## Library Monitoring
* [Deep Dive into Monitoring Spark Applications Using Web UI and SparkListeners (Jacek Laskowski)](https://www.youtube.com/watch?v=mVP9sZ6K__Y)
* [Apache Spark performance - All relevant key performance metrics about your Apache Spark instance in minutes](https://www.dynatrace.com/technologies/apache-spark-monitoring/apache-spark-performance/)
* [HTRACE TUTORIAL: HOW TO MONITOR YOUR DISTRIBUTED SYSTEMS](https://www.scalyr.com/blog/htrace-tutorial-how-to-monitor-distributed-systems)
* [delight: A Spark UI and Spark History Server alternative with CPU and Memory metrics! Delight is free, cross-platform, and open-source](https://github.com/datamechanics/delight)
  * [Delight: An Improved Apache Spark UI, Free, and Cross-Platform - YouTube](https://www.youtube.com/watch?v=9v03aztrVnA)
* [spark-dependencies - Spark job for dependency links http://jaegertracing.io ](https://github.com/jaegertracing/spark-dependencies)
* [spark-jobs-rest-client - Fluent client for interacting with Spark Standalone Mode's Rest API for submitting, killing and monitoring the state of jobs](https://github.com/ywilkof/spark-jobs-rest-client)
* [Sparklint - The missing Spark Performance Debugger that can be drag and dropped into your spark application!](https://github.com/groupon/sparklint)
  * [SparkLint: a Tool for Monitoring, Identifying and Tuning Inefficient Spark Jobs (Simon Whitear)](https://www.youtube.com/watch?v=reGerTzcvoA)
* [sparkoscope - Enabling Spark Optimization through Cross-stack Monitoring and Visualization](https://github.com/ibm-research-ireland/sparkoscope)
* [zipkin-dependencies - Spark job that aggregates zipkin spans for use in the UI](https://github.com/openzipkin/zipkin-dependencies)

# Machine Learning
* [BerkeleyX: CS190.1x Scalable Machine Learning](https://courses.edx.org/courses/BerkeleyX/CS190.1x/1T2015/)
  * [Spark: Cluster Computing with Working Sets](http://people.csail.mit.edu/matei/papers/2010/hotcloud_spark.pdf)
  * [Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)
* [Feature Engineering at Scale With Spark](http://eugenezhulenev.com/blog/2015/06/10/feature-engineering-at-scale/)
* [Audience Modeling With Spark ML Pipelines](http://eugenezhulenev.com/blog/2015/09/09/audience-modeling-with-spark-ml-pipelines/)
* [Spark + AI Summit 2018 — Overview](https://towardsdatascience.com/spark-ai-summit-2018-overview-7c5a8d7be296)
* [Using Native Math Libraries to Accelerate Spark Machine Learning Applications](https://www.cloudera.com/documentation/guru-howto/data_science/topics/ght_native_math_libs_to_accelerate_spark_ml.html)
  * Spark ML용 네이티브 라이브러리를 사용해 모델 훈련 속도를 높이는 방법
  * 네이티브 라이브러리가 Spark ML에 이점이 되는 이유
  * CDH Spark로 네이티브 라이브러리를 활성화하는 방법
  * 여타 네이티브 라이브러리 사용 시 Spark ML 성능과의 비교 분석
* [Machine Learning with Jupyter using Scala, Spark and Python: The Setup](https://medium.com/@faizanahemad/machine-learning-with-jupyter-using-scala-spark-and-python-the-setup-62d05b0c7f56)
* [Spark Day 2017 Machine Learning & Deep Learnig With Spark](https://www.slideshare.net/sanghoonlee982/machine-learning-deep-learnig-with-spark)
* [Building a Big Data Machine Learning Spark Application for Flight Delay Prediction](https://medium.com/@pedrodc/building-a-big-data-machine-learning-spark-application-for-flight-delay-prediction-4f9507cdb010)
* [Apache Spark 2.0 Preview: Machine Learning Model Persistence by Databricks](https://databricks.com/blog/2016/05/31/apache-spark-2-0-preview-machine-learning-model-persistence.html)
* [Ranking Algorithms for Spark Machine Learning Pipeline](https://github.com/oeegee/spark-ranking-algorithms) BM 25 + Wilson score on spark 2.2.0
* [An Introduction to Machine Learning with Apache Spark™](https://www.youtube.com/playlist?list=PLTPXxbhUt-YWBA6a9pS0aRsRxju0IfUeG)
* [Multiple Column Feature Transformations in Spark ML](http://blog.madhukaraphatak.com/multi-column-feature-transformation-spark-ml/)
* [End to End Spark TensorFlow PyTorch Pipelines with Databricks DeltaJim Dowling Logical Clocks ABKim](https://www.youtube.com/watch?v=zGNQQfEjCQY)
* [Accelerating Deep Learning on the JVM with Apache Spark and NVIDIA GPUs](https://www.infoq.com/articles/deep-learning-apache-spark-nvidia-gpu/)
* [Spark ML hyperparameter tuning](https://softwaremill.com/spark-ml-hyperparameter-tuning/)
* [Scaling and Unifying SciKit Learn and Apache Spark Pipelines - YouTube](https://www.youtube.com/watch?v=rr7C9tD8uZc)
* [Sawtooth Windows for Feature Aggregations - YouTube](https://www.youtube.com/watch?v=S3_ErDZWor0)
* [Run Your Queries Instantly in One of the Most Optimized Environments - YouTube](https://www.youtube.com/watch?v=ly7OqNCzFoQ) Nephos
* [KeystoneML - Machine Learning Pipeline](http://keystone-ml.org/)
* [Meson: Netflix's framework for executing machine learning workflows](http://techblog.netflix.com/2016/05/meson_31.html)
* MLflow
  * [Manage your Machine Learning Lifecycle with MLflow — Part 1](https://towardsdatascience.com/manage-your-machine-learning-lifecycle-with-mlflow-part-1-a7252c859f72)
  * [MLflow: An Open Platform to Simplify the Machine Learning Lifecycle](https://www.infoq.com/presentations/mlflow-databricks/)
* MLLib
  * [Decision Trees](http://spark.apache.org/docs/latest/mllib-decision-tree.html)
  * [MLlib: Machine Learning in Apache Spark](http://arxiv.org/pdf/1505.06807.pdf)
  * [movie recommendation with mllib](http://ampcamp.berkeley.edu/big-data-mini-course/movie-recommendation-with-mllib.html)
  * [WSO2 Machine Learner: Why would You care?](https://iwringer.wordpress.com/2015/09/25/wso2-machine-learner-why-would-you-care/)
  * [Strata 2016 - This repo is for MLlib/GraphX tutorial in Strata 2016](https://github.com/jayantshekhar/strata-2016)
  * [Spark ML Lab](https://github.com/Pivotal-Open-Source-Hub/StockInference-Spark/blob/master/SparkML.md)
  * [**Machine Learning with Spark (Spark로 머신러닝하기)**](https://moons08.github.io/programming/sparkML/)
  * [Apache Spark로 시작하는 머신러닝 입문](https://www.youtube.com/watch?v=PRLz11vv7VA)
    * [Apache Spark 입문에서 머신러닝까지](http://www.slideshare.net/DonamKim/apache-spark-64226109)
  * [Generating Recommendations at Amazon Scale with Apache Spark and Amazon DSSTNE](http://blogs.aws.amazon.com/bigdata/post/TxGEL8IJ0CAXTK/Generating-Recommendations-at-Amazon-Scale-with-Apache-Spark-and-Amazon-DSSTNE)
  * [Introduction to Machine Learning on Apache Spark MLlib](https://www.youtube.com/watch?v=qKYpMPPL-fo)
  * [Introduction to Machine learning with Spark](http://blog.madhukaraphatak.com/machine-learning-with-spark/)
    * [Introduction to Machine Learning with Spark](https://www.slideshare.net/datamantra/introduction-to-machine-learning-with-spark)
    * [Code and setup information for Introduction to Machine Learning with Spark](https://github.com/phatak-dev/introduction_to_ml_with_spark)
  * [Introduction to ML with Apache Spark MLib by Taras Matyashovskyy](https://www.youtube.com/watch?v=szpcW-SEJK4)
  * [pipelineio - End-to-End Spark ML and Tensorflow AI Data Pipelines](http://pipeline.io/)
  * [Extend Spark ML for your own model/transformer types](https://www.oreilly.com/learning/extend-spark-ml-for-your-own-modeltransformer-types)
  * [Accelerating Apache Spark MLlib with Intel® Math Kernel Library (Intel® MKL)](http://blog.cloudera.com/blog/2017/02/accelerating-apache-spark-mllib-with-intel-math-kernel-library-intel-mkl/)
  * [Improving BLAS library performance for MLlib](http://www.spark.tc/blas-libraries-in-mllib/)
  * [Extend Spark ML for your own model/transformer types](https://www.oreilly.com/learning/extend-spark-ml-for-your-own-modeltransformer-types)
  * [Machine Learning with Apache Spark](https://josepraveen.com/2018/02/04/machine-learning-with-apache-spark/)
  * [Building A Linear Regression with PySpark and MLlib](https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a)
  * [Building Custom ML PipelineStages for Feature Selection - Marc Kaminski](https://www.youtube.com/watch?v=iUNk-i5aFPY)
  * [Machine Learning with PySpark and MLlib — Solving a Binary Classification Problem](https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa)
  * [Dataset deduplication using spark’s MLlib](https://medium.com/@ronaldangel/deduplication-using-sparks-mllib-4a08f65e5ab9)
  * [Deep Learning with Apache Spark and TensorFlow](https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html)
  * [TensorFlow On Spark: Scalable TensorFlow Learning on Spark Clusters - Andy Feng & Lee Yang](https://www.youtube.com/watch?v=IxWfAcrZQhc)
    * [BigData와 결합한, 분산 Deep Learning 그 의미와 접근 방법에 대하여](http://hoondongkim.blogspot.com/2017/09/bigdata-distributed-deep-learning.html)
  * [github.com/yahoo/TensorFlowOnSpark](https://github.com/yahoo/TensorFlowOnSpark)
    * [Open Sourcing TensorFlowOnSpark: Distributed Deep Learning on Big-Data Clusters](http://yahoohadoop.tumblr.com/post/157196317141/open-sourcing-tensorflowonspark-distributed-deep)
  * [Deep learning for Apache Spark](https://www.oreilly.com/ideas/deep-learning-for-apache-spark)
  * [Spark machine learning & deep learning](https://www.slideshare.net/ssusere94328/spark-machine-learning-deep-learning)
  * [Spark Deep Learning Pipelines](https://www.facebook.com/nextobe1/posts/342192122883456)
  * Deep Learning With Apache Spark
    * [Part 1](https://towardsdatascience.com/deep-learning-with-apache-spark-part-1-6d397c16abd)
    * [Part 2](https://towardsdatascience.com/deep-learning-with-apache-spark-part-2-2a2938a36d35)
  * [Converting Spark ML Vector to Numpy Array](http://blog.madhukaraphatak.com/spark-vector-to-numpy/)
  * [PyData Tel Aviv Meetup: Learning Large Scale Models for Content Recommendation - Sonya Liberman](https://www.youtube.com/watch?v=6MpXPL4wJ4k)
* [MMLSpark - Microsoft Machine Learning for Apache Spark](https://github.com/Azure/mmlspark)
  * [Accelerated Spark on GPU-enabled clusters in Azure](https://azure.microsoft.com/en-us/blog/accelerated-spark-on-gpu-enabled-clusters-in-azure/?_lrsc=ff381697-5454-493b-9666-eaeaf066ba16)
* [Oryx 2: Lambda architecture on Apache Spark, Apache Kafka for real-time large scale machine learning http://oryx.io](https://github.com/OryxProject/oryx)
  * [Production Recommendation Systems with Cloudera](http://blog.cloudera.com/blog/2018/02/production-recommendation-systems-with-cloudera/) 기계 학습 기능을 위한 인프라 및 데이터 파이프라인을 구축하기 위해 Cloudera Oryx 프로젝트를 사용하는 예제
  * Kafka + Spark + Cloudera Hadoop 를 이용한 추천시스템
* [raydp: RayDP: Distributed data processing library that provides simple APIs for running Spark on Ray and integrating Spark with distributed deep learning and machine learning frameworks](https://github.com/oap-project/raydp)
  * [Build Large-Scale Data Analytics and AI Pipeline Using RayDP - YouTube](https://www.youtube.com/watch?v=B4iXQtxX2cs)
* [spark-vlbfgs - an implementation of the Vector-free L-BFGS solver and some scalable machine learning algorithms for Apache Spark](https://github.com/yanboliang/spark-vlbfgs)
* TransmogrifAI [Chetan Khatri - TransmogrifAI - Automate ML Workflow with power of Scala and Spark at massive scale](https://www.youtube.com/watch?v=96LLM2UI07o)

# Mesos
* [Spark + Mesos cluster mode, who uploads the jar?](http://stackoverflow.com/questions/33978672/spark-mesos-cluster-mode-who-uploads-the-jar)

# PySpark
* [PySpark](http://spark.apache.org/docs/latest/api/python/)
* [PySpark & Hadoop: 1) Ubuntu 16.04에 설치하기](https://beomi.github.io/2017/11/09/Install-PySpark-and-Hadoop-on-Ubuntu-16-04/)
* [PySpark & Hadoop: 2) EMR 클러스터 띄우고 PySpark로 작업 던지기](https://beomi.github.io/2017/11/27/EMR-and-PySpark/)
* [PySpark Cheat Sheet: Spark in Python](https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python)
* [Big Data Analytics using Python and Apache Spark | Machine Learning Tutorial](https://www.youtube.com/watch?v=5HWveDdrosk)
* troubleshooting
  * [A Beginner's Guide on Troubleshooting Spark Applications](http://www.slideshare.net/jcmia1/a-beginners-guide-on-troubleshooting-spark-applications)
  * `Caused by: java.lang.ClassNotFoundException: * org.elasticsearch.spark.package` sbt configuration such as resolvers
    * [Spark Runtime Error - ClassDefNotFound: SparkConf](http://stackoverflow.com/questions/31171825/spark-runtime-error-classdefnotfound-sparkconf)
  * `java.lang.OutOfMemoryError: GC overhead limit exceeded` increase driver memory
  * `org.apache.spark.SparkException: Could not find BlockManagerEndpoint1 or it has been stopped` 검색해도 특별히 나오는게 없음
  * `spark java.io.IOException: Filesystem closed` usually result RDD is too big
  * `Task not serializable`
    * [Spark - Task not serializable: How to work with complex map closures that call outside classes/objects?](http://stackoverflow.com/questions/23050067/spark-task-not-serializable-how-to-work-with-complex-map-closures-that-call-o)
    * [Task not serializable: java.io.NotSerializableException when calling function outside closure only on classes not objects](http://stackoverflow.com/questions/22592811/task-not-serializable-java-io-notserializableexception-when-calling-function-ou)
    * [java+spark: org.apache.spark.SparkException: Job aborted: Task not serializable: java.io.NotSerializableException](http://stackoverflow.com/questions/24046744/javaspark-org-apache-spark-sparkexception-job-aborted-task-not-serializable)
  * `TypeError: 'bool' object is not callable` Use `PYSPARK_PYTHON=...`
    * [Check Python version in worker before run PySpark job](https://issues.apache.org/jira/browse/SPARK-6216)
    * [spark-runs-in-local-but-not-in-yarn](http://stackoverflow.com/questions/28879803/spark-runs-in-local-but-not-in-yarn)
  * `yarn.scheduler.maximum.allocation-mb`
    * increase configuration for yarn-site.xml
    * empty disk (not enough free space may cause this too)
  * [Cannot submit Spark app to cluster, stuck on “UNDEFINED”](http://stackoverflow.com/questions/26883701/cannot-submit-spark-app-to-cluster-stuck-on-undefined)
    * `yarn.nodemanager.resource.memory-mb` 조정 후 동작 확인
  * `contains a task of very large size warning`
    * 문제; Dataframe으로 읽어 온 row들을 텍스트 처리 해서 row끼리 비교를 해야 하는데, a task of very large size warning 발생
    * 해결; 텍스트 처리 된 중간 결과물을 Redis에 저장한 뒤 별도 Spark 애플리케이션을 사용해서 Row by Row 처리
    * 원인
      * Spark는 각 Executor가 수행해야 할 작업을 Task라는 단위로 관리
      * RDD에 가해지는 연산을 상호 의존성에 따라 묶은 뒤 (Logical Planning) 여기에 최적화 룰을 적용해서 실제로 Executor가 처리해야 할 Task의 형태로 생성 (Physical Planning)
      * 이걸 내부 queue에 넣어 뒀다가 순차적으로 Executor에 보내서 처리
      * 이 과정을 좀 더 구체적으로 설명하자면, Driver 프로세스가 작업 루틴과 작업 대상 위치를 TaskDescription 객체로 만든 뒤 Serialize를 해서 Worker 프로세스에 네트워크 상으로 전송
      * 문제는 Task당 100kb를 넘으면 "contains a task of very large size warning" 경고 발생
      * 이 제한은 소스코드 안에 하드 코딩되어 있어 변경 불가능
      * broadcast 기능을 사용할 경우 상황은 더 악화
      * broadcast 기능은 task를 전송할 때와는 달리 데이터 값 그 자체를 Worker에 하나하나 보내는 방식으로 동작
      * 이 경우 보내야 할 row가 한두 개가 아니므로, 당연히 성능에 문제 발생
      * 이런 이유 때문에 자연어 처리가 된 중간 결과물을 별도 스토리지에 저장한 뒤 별도 애플리케이션에서 읽어와서 처리하는 방법만 가능
      * 여러 storage 중에서 굳이 Redis를 추천하는 이유는 빠르고, Key-Value Store라 관리하기 좋고, Sharding 기능 덕분에 읽기 분산도 잘 동작하기 때문
      * 최근 Spark ML에서 학습된 모델이 Redis에 저장되는 식으로 개발되고 있음
  * [Spark Interpreter 이슈 해결](http://arclab.tistory.com/128)
* [Getting started with PySpark - Part 1](http://www.mccarroll.net/blog/pyspark/index.html)
* [Getting started with PySpark - Part 2](http://www.mccarroll.net/blog/pyspark2/)
* [PySpark Internals](https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals)
* [Fast Data Analytics with Spark and Python](http://www.slideshare.net/BenjaminBengfort/fast-data-analytics-with-spark-and-python)
* [pyspark-hbase.py](https://gist.github.com/MLnick/6ec916b646c3004d7523)
* [Deploying PySpark on Red Hat Storage GlusterFS](http://redhatstorage.redhat.com/2015/02/17/deploying-pyspark-on-red-hat-storage-glusterfs/)
* [practice - weird case from pyspark-hbase (utf8 & unicode mixed)](https://gist.github.com/hyunjun/dea65972f3f723c0ad77)
* [Python Versus R in Apache Spark](http://www.datanami.com/2015/07/13/python-versus-r-in-apache-spark/)
* [biospark](https://github.com/biospin/biospark)
* [Plagiarizing and Paraphrasing Code From an Online Class for Content Marketing](http://minimaxir.com/2015/09/code-of-plagiarism/)
* [How-to: Use IPython Notebook with Apache Spark](http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/)
* [Configuring IPython Notebook Support for PySpark](http://ramhiser.com/2015/02/01/configuring-ipython-notebook-support-for-pyspark/)
* [pyADAM - This is a wrapper to load Parquet data in PySpark](https://github.com/arahuja/pyadam)
* [PySpark: 손상된 parquet파일 무시하기](https://beomi.github.io/2018/02/26/PySpark-Read-Parquet-ignoreCorruptedFiles/)
* [Accessing PySpark in PyCharm](http://renien.github.io/blog/accessing-pyspark-pycharm/)
* [pyspark-project-example - A simple example for PySpark based project](https://github.com/HyukjinKwon/pyspark-project-example)
* [Recommendation Systems for Implicit Feedback](https://github.com/csung7/Recommendation-Systems-for-Implicit-Feedback)
* [Hassle Free ETL with PySpark](https://www.youtube.com/watch?v=1L6wp7AxfPE)
* [안명호 : Python + Spark, 머신러닝을 위한 완벽한 결혼 - PyCon APAC 2016](https://www.youtube.com/watch?v=JEBNNE09JEQ)
* [Fully Arm Your Spark with Ipython and Jupyter in Python 3](https://mengdong.github.io/2016/08/08/fully-armed-pyspark-with-ipython-and-jupyter/)
  * [Installation](http://toree.apache.org/documentation/user/installation.html)
* [PySpark Cheat Sheet: Spark in Python](http://www.datasciencecentral.com/profiles/blogs/pyspark-cheat-sheet-spark-in-python)
* [**Apache Spark for Data Science**](https://www.youtube.com/playlist?list=PL0B_bv6Hd87dTd5890-nPKwl2JoqCATAf)
* [BigDL on CDH and Cloudera Data Science Workbench](http://blog.cloudera.com/blog/2017/04/bigdl-on-cdh-and-cloudera-data-science-workbench/) BigDL (Apache Spark의 심층 학습 라이브러리)을 워크 벤치와 함께 사용하는 방법
* [Distributed Deep Learning At Scale On Apache Spark With BigDL](https://www.slideshare.net/YuliaTell/distributed-deep-learning-at-scale-on-apache-spark-with-bigdl)
* [Deep Learning to Big Data Analytics on Apache Spark Using BigDL - Yuhao Yang & Xianyan Jia](https://www.youtube.com/watch?v=cqUvrs2PPOY)
* [Deep Learning on Qubole Using BigDL for Apache Spark – Part 2](https://www.qubole.com/blog/deep-learning-qubole-using-bigdl-apache-spark-part-2/)
  * 딥러닝 라이브러리인 BigDL을 사용하여 모델을 학습하고 평가하는 방법을 보여주는 간단한 자습서
* [Use your favorite Python library on PySpark cluster with Cloudera Data Science Workbench](http://blog.cloudera.com/blog/2017/04/use-your-favorite-python-library-on-pyspark-cluster-with-cloudera-data-science-workbench/) Python 라이브러리를 사용하는 PySpark 작업을 작성하는 방법
* [Install Spark on Windows (PySpark)](https://medium.com/@GalarnykMichael/install-spark-on-windows-pyspark-4498a5d8d66c)
* [pyspark 로컬 설치](https://jaeyung1001.tistory.com/87)
* [Get Started with PySpark and Jupyter Notebook in 3 Minutes](https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f)
* [**Best Practices Writing Production-Grade PySpark Jobs**](https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f)
  * [PySpark-Boilerplate](https://github.com/ekampf/PySpark-Boilerplate)
* [How to use PySpark on your computer](https://towardsdatascience.com/how-to-use-pyspark-on-your-computer-9c7180075617)
* [Spark Python Performance Tuning](http://stackoverflow.com/questions/27757117/spark-python-performance-tuning)
* [Getting The Best Performance With PySpark](https://www.slideshare.net/SparkSummit/getting-the-best-performance-with-pyspark)
* [Improving Python and Spark Performance and Interoperability: Spark Summit East talk by Wes McKinney](https://www.youtube.com/watch?v=qIKImANLFtE)
  * [Improving Python and Spark Performance and Interoperability: Spark Summit East talk by: Wes McKinney](https://www.slideshare.net/SparkSummit/improving-python-and-spark-performance-and-interoperability-spark-summit-east-talk-by-wes-mckinney)
* [High Performance Python On Spark](https://www.youtube.com/watch?v=abZ0f5ug18U)
  * [High Performance Python on Apache Spark](https://www.slideshare.net/wesm/high-performance-python-on-apache-spark)
* [Comparing Performance between Apache Spark and PySpark](https://medium.com/@sahandfarazzarrinkoub/comparing-performance-between-apache-spark-and-pyspark-63d68c067a55)
* [**Keynote: Making the Big Data ecosystem work together with Python - Holden Karau**](https://www.youtube.com/watch?v=LOHVKYtUO-o)
* [Downloading spark and getting started with python notebooks (jupyter) locally on a single computer](https://medium.com/explore-artificial-intelligence/downloading-spark-and-getting-started-with-python-notebooks-jupyter-locally-on-a-single-computer-98a76236f8c1)
* [A Brief Introduction to PySpark - A primer on PySpark for data science](https://towardsdatascience.com/a-brief-introduction-to-pyspark-ff4284701873)
* [Introducing Pandas UDF for PySpark](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html)
* [Reading CSV & JSON files in Spark – Word Count Example](http://kavita-ganesan.com/reading-csv-and-json-files-in-spark)
* [How to Upload/Download Files to/from Notebook in my Local machine](https://medium.com/ibm-data-science-experience/how-to-upload-download-files-to-from-notebook-in-my-local-machine-6a4e65a15767)
* [Analyze MongoDB Logs Using PySpark](https://medium.com/hackernoon/analyze-mongodb-logs-using-pyspark-97a915547da0)
* [Real-world Python workloads on Spark: EMR clusters](https://becominghuman.ai/real-world-python-workloads-on-spark-emr-clusters-3c6bda1a1350)
* [First Steps With PySpark and Big Data Processing](https://realpython.com/pyspark-intro/)
* [**New Pandas UDFs and Python Type Hints in the Upcoming Release of Apache Spark 3.0™**](https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html)
* [How to create a simple ETL Job locally with PySpark, PostgreSQL and Docker](https://itnext.io/how-to-create-a-simple-etl-job-locally-with-pyspark-postgresql-and-docker-ea53cd43311d)
* [Data Collab Lab: Automate Data Pipelines with PySpark SQL - YouTube](https://www.youtube.com/watch?v=QpVsP9Y7qIg)
* [Data Quality: Especially important with the medallion architecture with PySpark data testing - YouTube](https://www.youtube.com/watch?v=mZ33PJzJtlw)
* [How to Manage Python Dependencies in Spark - The Databricks Blog](https://databricks.com/blog/2020/12/22/how-to-manage-python-dependencies-in-pyspark.html)
* [데이터 분석 라이브러리 개발기 (1)](https://tech.devsisters.com/posts/devplay-analytics-library/)
* [데이터 분석 라이브러리 개발기 (2) - 통합 테스팅과 문서화를 동시에 잡는 방법](https://tech.devsisters.com/posts/testing-devplay-analytics-library/)
* [04b: Databricks – Spark SCD Type 2 with Merge | Java-Success.com](https://www.java-success.com/04b-databricks-spark-scd-type-2-with-merge/)
* [웹로그 히스토리 데이터를 통한 데이터 분석 꼼수 : 네이버 블로그](https://blog.naver.com/airguy76/222140845122)
* [Pandas API on Apache Spark - Part 1: Introduction](http://blog.madhukaraphatak.com/spark-pandas-part-1/)
* [Pandas API on Apache Spark - Part 2: Hello World](http://blog.madhukaraphatak.com/spark-pandas-part-2/)
* [Simplifying Testing of Spark Applications - Megan Yow | PyData Global 2021 - YouTube](https://www.youtube.com/watch?v=_ieqg_soB3U)
* [Pyspark Functions - YouTube](https://www.youtube.com/playlist?list=PLgPb8HXOGtsQwAghLQWviaisvzgWN_Ix9)
* [How to Convert Pandas DataFrame to Spark DataFrame | Using PySpark - YouTube](https://www.youtube.com/watch?v=TG0-yPJPTXU)
* [How PySpark Self-Join Simplifies Data Flattening - YouTube](https://www.youtube.com/watch?v=bkNeOjJ8N50)
* [Koalas: pandas API on Apache Spark](https://github.com/databricks/koalas)
  * [Koalas: Easy Transition from pandas to Apache Spark](https://databricks.com/blog/2019/04/24/koalas-easy-transition-from-pandas-to-apache-spark.html)
  * [10 Minutes from pandas to Koalas on Apache Spark With demonstrable Python how-to Koalas code snippets and Koalas best practices](https://databricks.com/blog/2020/03/31/10-minutes-from-pandas-to-koalas-on-apache-spark.html)
  * [New Developments in the Open Source Ecosystem: Apache Spark 3 0, Delta Lake, and Koalas](https://www.youtube.com/watch?v=scM_WQMhB3A)
  * [pandas 코드로 대규모 클러스터에서 더 빠르게 빅데이터를 분석 해보자 - Koalas - 박현우 - PyCon Korea 2020 - YouTube](https://www.youtube.com/watch?v=Y9kdUq_qIa8)
  * [The Jungle of Koalas, Pandas, Optimus and Spark | by Favio Vázquez | Towards Data Science](https://towardsdatascience.com/the-jungle-of-koalas-pandas-optimus-and-spark-dd486f873aa4)
  * [Project Zen: Making Spark Pythonic | Reynold Xin | Keynote Data + AI Summit EU 2020 - YouTube](https://www.youtube.com/watch?v=-vJLTEOdLvA)
* [Petastorm - a library enabling the use of Parquet storage from Tensorflow, Pytorch, and other Python-based ML training frameworks](https://petastorm.readthedocs.io/)
  * [Introducing Petastorm: Uber ATG's Data Access Library for Deep Learning | Uber Engineering Blog](https://eng.uber.com/petastorm/)
* Snowflake
  * [Read from Kafka & Write to Snowflake via Spark Databricks | LinkedIn](https://www.linkedin.com/pulse/read-from-kafka-write-snowflake-via-spark-databricks-deepak-rajak/?trackingId=nV2nP1ws2QRWZVSBOpNkDw%3D%3D)
  * [입 개발 Spark SQL Query to Snowflake Query | Charsyam's Blog](https://charsyam.wordpress.com/2021/09/11/%ec%9e%85-%ea%b0%9c%eb%b0%9c-spark-sql-query-to-snowflake-query/)

# R
* [Spark 1.4 for RStudio](http://www.r-bloggers.com/spark-1-4-for-rstudio/)
* [Python Versus R in Apache Spark](http://www.datanami.com/2015/07/13/python-versus-r-in-apache-spark/)
* [SparkR 설치 사용기 1 - Installation Guide On Yarn Cluster & Mesos Cluster & Stand Alone Cluster](http://hoondongkim.blogspot.com/2016/01/sparkr-1-installation-guide-on-yarn.html)
* [MS R(구 Revolution R) on Spark - 설치 및 가능성 엿보기(feat. SparkR)](http://hoondongkim.blogspot.com/2016/12/ms-r-revolution-r-on-spark-feat-sparkr.html)
* [sparklyr — R interface for Apache Spark](http://spark.rstudio.com/index.html)
* [sparklyr — R interface for Apache Spark](https://blog.rstudio.org/2016/09/27/sparklyr-r-interface-for-apache-spark/)
* [sparklyr](https://drive.google.com/file/d/0Bw594TdiBdAUUWt6eGd0Vm5fWDg/view)
* [xwMOOC 기계학습 - dplyr을 Spark 위에 올린 sparklyr](http://statkclee.github.io/ml/ml-sparklyr.html)
* [sparklyr – An  R interface for Apache Spark](https://cdn.oreillystatic.com/en/assets/1/event/193/Sparklyr_%20An%20R%20interface%20for%20Apache%20Spark%20Presentation.pdf)
* [spark + R](https://drive.google.com/file/d/0Bw594TdiBdAUTGtUOERoOG1ac1E/view)
* [빅데이터 분석을 위한 스파크 2 프로그래밍 : 대용량 데이터 처리부터 머신러닝까지](http://www.slideshare.net/ssuser88c366/2-71401153)
* [On-Demand Webinar and FAQ: Parallelize R Code Using Apache Spark](https://databricks.com/blog/2017/08/21/on-demand-webinar-and-faq-parallelize-r-code-using-apache-spark.html)
* [Vectorized R Execution in Apache Spark - Hyukjin Kwon (Databricks)](https://www.youtube.com/watch?v=3fE9MrV7uqA)
* [How to Improve R Performance in SparkR at Apache Spark 3.0](https://databricks.com/blog/2020/06/01/vectorized-r-i-o-in-upcoming-apache-spark-3-0.html)

# Spark 3.0
* [Data Source V2 API in Spark 3.0 - Part 1 : Motivation for New Abstractions](http://blog.madhukaraphatak.com/spark-3-datasource-v2-part-1/)
* [Data Source V2 API in Spark 3.0 - Part 2 : Anatomy of V2 Read API](http://blog.madhukaraphatak.com/spark-3-datasource-v2-part-2/)
* [Data Source V2 API in Spark 3.0 - Part 3 : In-Memory Data Source](http://blog.madhukaraphatak.com/spark-3-datasource-v2-part-3/)
* [Data Source V2 API in Spark 3.0 - Part 4 : In-Memory Data Source with Partitioning](http://blog.madhukaraphatak.com/spark-3-datasource-v2-part-4/)
* [Data Source V2 API in Spark 3.0 - Part 5 : Anatomy of V2 Write API](http://blog.madhukaraphatak.com/spark-3-datasource-v2-part-5/)
* [Data Source V2 API in Spark 3.0 - Part 6 : MySQL Source](http://blog.madhukaraphatak.com/spark-3-datasource-v2-part-6/)
* [Introduction to Spark 3.0 - Part 1 : Multi Character Delimiter in CSV Source](http://blog.madhukaraphatak.com/spark-3-introduction-part-1/)
* [Introduction to Spark 3.0 - Part 2 : Multiple Column Feature Transformations in Spark ML](http://blog.madhukaraphatak.com/spark-3-introduction-part-2/)
* [Introduction to Spark 3.0 - Part 3 : Data Loading From Nested Folders](http://blog.madhukaraphatak.com/spark-3-introduction-part-3/)
* [Introduction to Spark 3.0 - Part 4 : Handling Class Imbalance Using Weights](http://blog.madhukaraphatak.com/spark-3-introduction-part-4/)
* [Introduction to Spark 3.0 - Part 5 : Easier Debugging of Cached Data Frames](http://blog.madhukaraphatak.com/spark-3-introduction-part-5/)
* [Introduction to Spark 3.0 - Part 6 : Min and Max By Functions](http://blog.madhukaraphatak.com/spark-3-introduction-part-6/)
* [Introduction to Spark 3.0 - Part 7 : Dynamic Allocation Without External Shuffle Service](http://blog.madhukaraphatak.com/spark-3-introduction-part-7/)
* [Introduction to Spark 3.0 - Part 8 : DataFrame Tail Function](http://blog.madhukaraphatak.com/spark-3-introduction-part-8/)
* [Introduction to Spark 3.0 - Part 9 : Join Hints in Spark SQL](http://blog.madhukaraphatak.com/spark-3-introduction-part-9/)
* [Introduction to Spark 3.0 - Part 10 : Ignoring Data Locality in Spark](http://blog.madhukaraphatak.com/spark-3-introduction-part-10/)
* [Spark Plugin Framework in 3.0 - Part 1: Introduction](http://blog.madhukaraphatak.com/spark-plugin-part-1/)
* [Spark Plugin Framework in 3.0 - Part 2 : Anatomy of the API](http://blog.madhukaraphatak.com/spark-plugin-part-2/)
* [Spark Plugin Framework in 3.0 - Part 3 : Dynamic Stream Configuration using Driver Plugin](http://blog.madhukaraphatak.com/spark-plugin-part-3/)
* [Spark Plugin Framework in 3.0 - Part 4 : Custom Metrics](http://blog.madhukaraphatak.com/spark-plugin-part-4/)
* [Spark Plugin Framework in 3.0 - Part 5 : RPC Communication](http://blog.madhukaraphatak.com/spark-plugin-part-5/)
* [Adaptive Query Execution in Spark 3.0 - Part 1 : Introduction](http://blog.madhukaraphatak.com/spark-aqe-part-1/)
* [Adaptive Query Execution in Spark 3.0 - Part 2 : Optimising Shuffle Partitions](http://blog.madhukaraphatak.com/spark-aqe-part-2/)
* [AQE: Coalescing Post Shuffle Partitions – tech.kakao.com](https://tech.kakao.com/2022/01/18/aqe-coalescing-post-shuffle-partitions/)
* [Distributed TensorFlow on Apache Spark 3.0](http://blog.madhukaraphatak.com/tensorflow-on-spark-3.0/)
* [Barrier Execution Mode in Spark 3.0 - Part 1 : Introduction](http://blog.madhukaraphatak.com/barrier-execution-mode-part-1/)
* [Barrier Execution Mode in Spark 3.0 - Part 2 : Barrier RDD](http://blog.madhukaraphatak.com/barrier-execution-mode-part-2/)
* [Webinar: A preview of Apache Spark 3.0](https://www.youtube.com/watch?v=g-qZslQsOuE)
* [Spark & AI summit and a glimpse of Spark 3.0 - Towards Data Science](https://towardsdatascience.com/spark-ai-summit-and-a-glimpse-of-spark-3-0-5fe0775386de)
* [Spark 3.0에 새로 추가된 기능 소개 및 설명 - Nephtyw’S Programming Stash](https://nephtyws.github.io/data/whats-new-in-spark-3/)
* [NVIDIA Accelerates Spark Data Analytics Platform | NVIDIA Blog](https://blogs.nvidia.com/blog/2020/06/24/apache-spark-gpu-acceleration/)
* [Spark 3.0 — New Functions in a Nutshell - Javarevisited - Medium](https://medium.com/javarevisited/spark-3-0-new-functions-in-a-nutshell-a929fca93413)
* [Spark & AI summit and a glimpse of Spark 3.0 | by Adi Polak | Towards Data Science](https://towardsdatascience.com/spark-ai-summit-and-a-glimpse-of-spark-3-0-5fe0775386de)
* [Apache Spark 3.0 변경 사항](https://lightningdb.io/blog/2020/10/apache-spark-3.0-review.html)

# Spark DL
* [A Vision for Making Deep Learning Simple From Machine Learning Practitioners to Business Analysts](https://databricks.com/blog/2017/06/06/databricks-vision-simplify-large-scale-deep-learning.html)

# Spark SQL
* [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)
* [Deep Dive into Spark SQL’s Catalyst Optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)
  * DataFrame이 RDD와 다르게 최적화를 적용할 수 있는 이유
* [SparkSQL cacheTable 메소드 사용 성능 비교 - default vs cacheTable vs cacheTable (with columnar Compression)](http://hoondongkim.blogspot.kr/2015/04/sparksql-cachetable-default-vs.html?spref=fb)
* [SparkSQL Internals](http://www.trongkhoanguyen.com/2015/08/sparksql-internals.html)
* [Spark Data Source API. Extending Our Spark SQL Query Engine](https://hackernoon.com/extending-our-spark-sql-query-engine-5f4a088de986)
* [Five Spark SQL Utility Functions to Extract and Explore Complex Data Types](https://databricks.com/blog/2017/06/13/five-spark-sql-utility-functions-extract-explore-complex-data-types.html)
  * JSON 및 중첩 구조를 처리하기 위해 탑재된 Spark SQL 함수를 사용하기 위한 튜토리얼
* [Spark SQL: Another 16x Faster After Tungsten](https://databricks.com/session/spark-sql-another-16x-faster-after-tungsten)
* [Windowing Functions in Spark SQL Part 1 | Lead and Lag Functions | Windowing Functions Tutorial](https://www.youtube.com/watch?v=MViPRjaqfaA)
* [Windowing Functions in Spark SQL Part 2 | First Value & Last Value Functions | Window Functions](https://www.youtube.com/watch?v=tsQ2oQieBGY)
* [Windowing Functions in Spark SQL Part 3 | Aggregation Functions | Windowing Functions Tutorial](https://www.youtube.com/watch?v=pNEU3SkfyAw)
* [Windowing Functions in Spark SQL Part 4 | Row_Number, Rank and Dense_Rank in SQL](https://www.youtube.com/watch?v=hoOwDUrodhM)
* [Simplifying Change Data Capture with Databricks Delta](https://databricks.com/blog/2018/10/29/simplifying-change-data-capture-with-databricks-delta.html)
* [Spark DataFrameWriter에서 saveAsTable 의 동작](https://charsyam.wordpress.com/2019/04/23/%EC%9E%85-%EA%B0%9C%EB%B0%9C-spark-dataframewriter%EC%97%90%EC%84%9C-saveastable-%EC%9D%98-%EB%8F%99%EC%9E%91/)
* [Dynamic Shuffle Partitions in Spark SQL](http://blog.madhukaraphatak.com/dynamic-spark-shuffle-partitions/)
* [Tech Chat: Faster Spark SQL: Adaptive Query Execution in Databricks - YouTube](https://www.youtube.com/watch?v=bQ33bwUE-ms)
* [Sentiment Analysis on Demonetization in India using Apache Spark - Projects Based Learning](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)
* [FLARE: SCALE UP SPARK SQL WITH NATIVE COMPILATION AND SET YOUR DATA ON FIRE!](https://spark-summit.org/2017/events/flare-scale-up-spark-sql-with-native-compilation-and-set-your-data-on-fire/)
  * 실험 단계
  * 쿼리플랜을 native code로 바꾸고 spark runtime system도 수정해 Spark SQL성능을 대폭 향상
  * [Flare: Native Compilation for Heterogeneous Workloads in Apache Spark](https://arxiv.org/pdf/1703.08219.pdf)
* [MatFast: In-Memory Distributed Matrix Computation Processing and Optimization Based on Spark SQL](https://databricks.com/session/matfast-in-memory-distributed-matrix-computation-processing-and-optimization-based-on-spark-sql)
  * [Apache Spark™ Distributed Matrix Computation](https://github.com/yuyongyang800/SparkDistributedMatrix)

# Streaming
* [Improved Fault-tolerance and Zero Data Loss in Spark Streaming](https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html)
* [Four Things to know about Reliable Spark Streaming](http://www.typesafe.com/resources/video/four-things-to-know-about-reliable-spark-streaming)
* [Improved Fault-tolerance and Zero Data Loss in Spark Streaming](https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html)
* [Real Time Data Processing using Spark Streaming | Data Day Texas 2015](http://www.slideshare.net/cloudera/spark-streamingdatadaytexas-43476479)
* [Real-Time Analytics with Spark Streaming](http://qconsp.com/sp2015/system/files/presentation-slides/QCon_paco.ok_.pdf)
  * [Diving into Spark Streaming’s Execution Model](https://databricks.com/blog/2015/07/30/diving-into-spark-streamings-execution-model.html)
* [Can Spark Streaming survive Chaos Monkey?](http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html)
* [RecoPick 실시간 데이터 처리 시스템 전환기 (Storm에서 Spark Streaming으로 전환)](http://readme.skplanet.com/?p=13297)
* [From Big Data to Fast Data in Four Weeks or How Reactive Programming is Changing the World – Part 2](https://www.paypal-engineering.com/2016/11/18/from-big-data-to-fast-data-in-four-weeks-or-how-reactive-programming-is-changing-the-world-part-2/)
* [Spark Streaming으로 유실 없는 스트림 처리 인프라 구축하기](http://readme.skplanet.com/?p=12465)
* [Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1](https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html)
* [Handling empty batches in Spark streaming](http://blog.madhukaraphatak.com/handling-empty-rdd-in-spark-streaming/)
* [Spark Streaming Example(예제로 알아보는 Spark Streaming)](http://hellowuniverse.com/2017/03/23/spark-streaming-example%EC%98%88%EC%A0%9C%EB%A1%9C-%EC%95%8C%EC%95%84%EB%B3%B4%EB%8A%94-spark-streaming/)
* [Long-running Spark Streaming Jobs on YARN Cluster](http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/)
  * spark-submit으로 장기간 streaming 분석 작업 실행하기
* [Spark Streaming 운영과 회고](http://slides.com/yonghweekim/streaming-system#/)
* [Deep Learning and Streaming in Apache Spark 2 x - Matei Zaharia & Sue Ann Hong](https://www.youtube.com/watch?v=zom9J9sK6wY)
* [24/7 Spark Streaming on YARN in Production](https://www.inovex.de/blog/247-spark-streaming-on-yarn-in-production/)
* [Running multiple Spark Streaming jobs of different DStreams in parallel](https://stackoverflow.com/questions/43167592/running-multiple-spark-streaming-jobs-of-different-dstreams-in-parallel)
* [Arbitrary Stateful Processing in Apache Spark’s Structured Streaming](https://databricks.com/blog/2017/10/17/arbitrary-stateful-processing-in-apache-sparks-structured-streaming.html)
  * 'exactly once' 주제에서 Apache Spark의 Structured Streaming으로 중복 제거를 구현하는 방법에 대해 설명
  * 워터마크 기반으로 한 중복 제거 외에도 mapGroupsWithState를 사용하여 상태 저장 집계에 사용자 정의 로직을 추가 할 수 있는 방법에 대해 간략하게 설명
* [Internals of Spark Streaming](http://bytecontinnum.com/ioss/)
* [Why is My Stream Processing Job Slow?](https://databricks.com/session/why-is-my-stream-processing-job-slow)
* [How we built a data pipeline with Lambda Architecture using Spark/Spark Streaming](https://medium.com/walmartlabs/how-we-built-a-data-pipeline-with-lambda-architecture-using-spark-spark-streaming-9d3b4b4555d3) 월마트 랩에서 Apache Kafka, Spark Streaming/Batch로 Lambda 아키텍처를 구현하기 위해 구축된 A/B 테스트 플랫폼 소개
* [Building Realtime Data Pipelines with Kafka Connect and Spark Streaming](https://www.youtube.com/watch?v=wMLAlJimPzk)
* [Ingesting Raw Data with Kafka-connect and Spark Datasets](https://medium.com/swlh/ingesting-raw-data-with-kafka-connect-and-spark-datasets-1c2b7aa9ba3b)
* [Introduction to Spark Structured Streaming - Part 15: Meetup Talk on Time and Window API](http://blog.madhukaraphatak.com/introduction-to-spark-structured-streaming-part-15/)
* [번역글 Spark Streaming의 내부](https://www.popit.kr/%EB%B2%88%EC%97%AD%EA%B8%80-spark-streaming%EC%9D%98-%EB%82%B4%EB%B6%80/)
* [**Comparing Apache Spark, Storm, Flink and Samza stream processing engines - Part 1**](https://blog.scottlogic.com/2018/07/06/comparing-streaming-frameworks-pt1.html) Apache Spark, Storm, Flink, Samze를 비교 분석
* [Kafka Streams vs. Spark Structured Streaming](https://speakerdeck.com/dongjin/kafka-streams-vs-spark-structured-streaming)
* [Kafka Streams vs. Spark Structured Streaming (extended)](https://speakerdeck.com/dongjin/kafka-streams-vs-spark-structured-streaming-extended)
* [Kafka offset committer for Spark structured streaming](https://github.com/HeartSaVioR/spark-sql-kafka-offset-committer)
  * Structured Streaming은 Kafka 에서 데이터를 가져올 때 사용하는 경우가 많음
  * Spark가 Kafka consumer group ID를 임의로 지정하고 commit도 하지 않아 별도의 streaming query listener를 구현해 추적하는 방안 외에는 적당한 방도가 없음
  * commit할 group ID를 지정하면 개별 batch의 commit된 offset정보를 Kafka로 commit, 기존 Kafka 툴들과 조합하면 lag등을 추적하는 데 도움
* [Scaling Spark Streaming for Logging Event Ingestion](https://medium.com/airbnb-engineering/scaling-spark-streaming-for-logging-event-ingestion-4a03141d135d)
* [State Storage in Spark Structured Streaming](https://medium.com/@polarpersonal/state-storage-in-spark-structured-streaming-e5c8af7bf509)
* [State Management in Spark Structured Streaming](https://medium.com/@chandanbaranwal/state-management-in-spark-structured-streaming-aaa87b6c9d31)
* [Watermarking in Spark Structured Streaming](https://towardsdatascience.com/watermarking-in-spark-structured-streaming-9e164f373e9)
* [Structured streaming in a flash](https://medium.com/@johankok/structured-streaming-in-a-flash-576cdb17bbee)
* [File sink and Out-Of-Memory risk on waitingforcode.com - articles about Apache Spark Structured Streaming](https://www.waitingforcode.com/apache-spark-structured-streaming/file-sink-out-of-memory-risk/read)
* [입 개발 Kafka 와 Spark Structured Streaming 에서 checkpoint 에서 아주 과거의 Offset이 있으면 어떻게 동작할까? | Charsyam's Blog](https://charsyam.wordpress.com/2021/03/08/%EC%9E%85-%EA%B0%9C%EB%B0%9C-kafka-%EC%99%80-spark-structured-streaming-%EC%97%90%EC%84%9C-checkpoint-%EC%97%90%EC%84%9C-%EC%95%84%EC%A3%BC-%EA%B3%BC%EA%B1%B0%EC%9D%98-offset%EC%9D%B4-%EC%9E%88/)
* [입 개발 Spark Structured Streaming 에서 Offset 은 어떻게 관리되는가(아주 간략한 버전)? | Charsyam's Blog](https://charsyam.wordpress.com/2021/03/09/%EC%9E%85-%EA%B0%9C%EB%B0%9C-spark-structured-streaming-%EC%97%90%EC%84%9C-offset-%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EA%B4%80%EB%A6%AC%EB%90%98%EB%8A%94%EA%B0%80%EC%95%84%EC%A3%BC-%EA%B0%84/)
* [입 개발 Spark Kafka Streaming 에서의 BackPressure 에 대한 아주 간단한 정리. | Charsyam's Blog](https://charsyam.wordpress.com/2021/04/24/%EC%9E%85%EA%B0%9C%EB%B0%9C-spark-kafka-streaming-%EC%97%90%EC%84%9C%EC%9D%98-backpressure-%EC%97%90-%EB%8C%80%ED%95%9C-%EC%95%84%EC%A3%BC-%EA%B0%84%EB%8B%A8%ED%95%9C-%EC%A0%95%EB%A6%AC/)
* [Structured Streaming Use-Cases at Apple - YouTube](https://www.youtube.com/watch?v=bcIJFCtsRXs)

# TDD, Test
* [Unit Testing Apache Spark Applications using Hive Tables](https://medium.com/homeaway-tech-blog/unit-testing-apache-spark-applications-using-hive-tables-ec653c6f25be)
* [How I test with Apache Spark?](https://medium.com/@nastasia.saby/how-i-test-with-apache-spark-97b2bacf0b77)

# YARN
* [Running Spark on YARN](http://spark.apache.org/docs/latest/running-on-yarn.html)
* [Apache Spark Resource Management and YARN App Models](http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/)
* [Spark-on-YARN: Empower Spark Applications on Hadoop Cluster](http://www.slideshare.net/Hadoop_Summit/sparkonyarn-empower-spark-applications-on-hadoop-cluster)
* [Spark Yarn Cluster vs Spark Mesos Cluster (vs 기타 다양한 모드) 수행성능 및 활용성 비교](http://hoondongkim.blogspot.kr/2015/10/spark-yarn-cluster-vs-spark-mesos.html)
* [Dynamic Resource Allocation Spark on YARN](http://www.slideshare.net/ozax86/spark-on-yarn-with-dynamic-resource-allocation)
* [Investigation of Dynamic Allocation in Spark](http://jerryshao.me/architecture/2015/08/22/spark-dynamic-allocation-investigation/)
* [Spark Cluster Settings On Yarn : Spark 1.4.1 + Hadoop 2.7.1](http://hoondongkim.blogspot.com/2015/08/spark-cluster-settings-on-yarn-spark.html)
* [Spark logging configuration in YARN](https://medium.com/@iacomini.riccardo/spark-logging-configuration-in-yarn-faf5ba5fdb01)
* [Understanding Apache Spark on YARN](http://sujithjay.com/2018/07/24/Understanding-Apache-Spark-on-YARN/)
* [Spark on YARN: a Deep Dive - Sandy Ryza, Cloudera](https://www.youtube.com/watch?v=N6pJhxCPe-Y)
* [Apache Spark Performance Benchmarks show Kubernetes has caught up with YARN - Data Mechanics Blog](https://datamechanics.webflow.io/blog-post/apache-spark-performance-benchmarks-show-kubernetes-has-caught-up-with-yarn)

# Zeppelin
* [Zeppelin](http://zeppelin-project.org/)
* [Apache Zeppelin Release 0.7.0](http://zeppelin.apache.org/releases/zeppelin-release-0.7.0.html)
* [www.zepl.com](https://www.zepl.com/) previously www.zeppelinhub.com
* practice
  * [meetup](https://github.com/hyunjun/practice/tree/master/spark/meetup)
* [Introduction to Zeppelin](http://www.slideshare.net/KSLUG/kslug-zeppelin)
* [Zeppelin overview](https://www.youtube.com/watch?v=_PQbVH_aO5E)
* [Zepplin (제플린) 설치하기](http://bcho.tistory.com/1022)
* [도커로 간단 설치하는 Zeppelin](https://docs.google.com/presentation/d/1iUlprfqeQaXuW63qQpb7eHkV3oiegtl3OOylHpX6dGg/edit)
* [5. 웹 기반 명령어 해석기 Zeppelin Install](http://pubdata.tistory.com/28)
* [How-to: Install Apache Zeppelin on CDH](http://blog.cloudera.com/blog/2015/07/how-to-install-apache-zeppelin-on-cdh/)
* [Angular display system dashboard on Zeppelin](https://www.youtube.com/watch?v=QdjZyOkcG_w)
* [Apache Zeppelin으로 데이터 분석하기 by VCNC](https://speakerdeck.com/vcnc/apache-zeppelineuro-deiteo-bunseoghagi)
* [Zeppelin Context](http://zeppelin-project.org/docs/zeppelincontext.html)
* [Apache Tajo 데스크탑 + Zeppelin 연동 하기](http://jjeong.tistory.com/1031)
* [제플린 탑재한 이엠알 16년 4월](http://www.slideshare.net/diginorimin/16-4-60944385)
* [Zeppelin at Twitter](http://www.slideshare.net/prasadwagle/zeppelin-at-twitter-62171116)
* [아파치 제플린, 한국에서 세계로 가기까지](http://m.zdnet.co.kr/news_view.asp?article_id=20160601155438)
* [Zeppelin Lab](https://github.com/Pivotal-Open-Source-Hub/StockInference-Spark/blob/master/Zeppelin.md)
* [Presto, Zeppelin을 이용한 초간단 BI 구축 사례](http://www.slideshare.net/babokim/presto-zeppelin-bi)
* [Presto, Zeppelin을 이용한 초간단 BI 시스템 구축 사례(1)](http://www.popit.kr/presto-zeppelin%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%B4%88%EA%B0%84%EB%8B%A8-bi-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EA%B5%AC%EC%B6%95-%EC%82%AC%EB%A1%80-1/)
* [Serving Shiro enabled Apache Zeppelin with Apache mod_proxy + SSL (https)](https://nazgul33.wordpress.com/2016/08/31/serving-shiro-enabled-apache-zeppelin-with-apache-mod_proxy-ssl-https/)
* [Analyzing BigQuery datasets using BigQuery Interpreter for Apache Zeppelin](https://cloud.google.com/blog/big-data/2016/09/analyzing-bigquery-datasets-using-bigquery-interpreter-for-apache-zeppelin)
* [Zeppelin(제플린) 서울시립대학교 데이터 마이닝연구실 활용사례](http://www.slideshare.net/JunKim22/zeppelin-66264643)
  * [제플린 걸음마 서울시립대학교 데이터마이닝 활용사례 제플린 노트북 통계 추출 코드](https://gist.github.com/tae-jun/138f595228aa83e89387b5d39d33b315)
* [노트7의 소셜 반응을 분석해 보았다. #3 제플린 노트북을 이용한 상세 분석](http://bcho.tistory.com/1138)
* [9월 발렌타인 웨비너 - 민경국님의 Apache Zeppelin 입문 온라인 헨즈온강의](https://www.youtube.com/watch?v=VlqTPZVyP9Y)
* [오픈소스 일기 2: Apache Zeppelin 이란 무엇인가?](https://medium.com/apache-zeppelin-stories/%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4-%EC%9D%BC%EA%B8%B0-2-apache-zeppelin-%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80-f3a520297938)
* [How Apache Zeppelin runs a paragraph](https://medium.com/apache-zeppelin-stories/how-apache-zeppelin-runs-a-paragraph-783a0a612ba9)
* [**Spark & Zeppelin을 활용한 머신러닝 실전 적용기**](http://www.slideshare.net/JunKim22/spark-zeppelin)
  * [Zeppelin 화재 뉴스 기사 분류 예제](https://github.com/uosdmlab/playdata-zeppelin-notebook)
* [스파크-제플린으로 통계 그래프 출력하기(윈도우환경)](http://blog.daum.net/web_design/396) 실패 이야기
* [Apache Zeppelin Data Science Environment 1/21/16](https://www.youtube.com/watch?v=chPw8Ts7ZW8)
* [Zeppelin Build and Tutorial Notebook](https://www.youtube.com/watch?v=CfhYFqNyjGc)
* [zdairi is zeppelin CLI tool](https://github.com/del680202/zdairi)
* [Zeppelin Paragraph 공유 시 자동 로그인 구현](http://www.popit.kr/zeppelin-paragraph-%EA%B3%B5%EC%9C%A0-%EC%8B%9C-%EC%9E%90%EB%8F%99-%EB%A1%9C%EA%B7%B8%EC%9D%B8-%EA%B5%AC%ED%98%84/)
* [25분 만에 Apache Zeppelin 으로 대시보드 만들기 - 박훈(@1ambda)](https://www.youtube.com/watch?v=VKMB8nFhjug)
* [Using Amazon Athena with Apache Zeppelin](https://medium.com/@yutaimai/using-amazon-athena-with-apache-zeppelin-464a85678c46)
* [ZEPL - How to Configure a JDBC Interpreter](https://www.youtube.com/watch?v=sgdgFPnE7Lw)
* [www.zepl.com/resources](https://www.zepl.com/resources/) how-to videos
* [Spark Scala Note 1](https://medium.com/jk-link/spark-scala-note-1-1c721da15b26)
* [Journey to the Continuous and Scalable Big Data Platform](https://www.slideshare.net/blrunner/journey-to-the-continuous-and-scalable-big-data-platform)
* [Big Data Tools 소개 – IntelliJ IDEA 내에서 Spark 통합 및 Zeppelin 노트북 지원](https://blog.jetbrains.com/kr/2019/10/big-data-tools-%EC%86%8C%EA%B0%9C-intellij-idea-%EB%82%B4%EC%97%90%EC%84%9C-spark-%ED%86%B5%ED%95%A9-%EB%B0%8F-zeppelin-%EB%85%B8%ED%8A%B8%EB%B6%81-%EC%A7%80%EC%9B%90/)
* [K-Means clustering with Apache Spark and Zeppelin notebook on Docker](https://medium.com/rahasak/k-means-clustering-with-apache-spark-and-zeppelin-notebook-on-docker-4ed2db66c3c8)
* [Zeppelin notebook shortcuts - Mk’s Blog](https://moons08.github.io/programming/zeppelin_shortcuts/)
* [Using Apache Zeppelin with SQL Server | by Mike Moritz | Medium](https://medium.com/@mike.p.moritz/using-apache-zeppelin-with-sql-server-d1964207ac5e)
* [Zeplin ML: a ML plugin for Zeplin - YouTube](https://www.youtube.com/watch?v=a3jANIGk5EA)
* [📊데이터 시각화 플랫폼 제플린 #zeppelin #dataviz - YouTube](https://www.youtube.com/watch?v=_7EP_FYVR-s)
  * [zeppelin](https://okdevtv.com/mib/zeppelin)
* [📊제플린 쉽게 시작하기 #zeppelin #dataviz - YouTube](https://www.youtube.com/watch?v=y-GWhTNW3sk)
* [📊제플린과 DB 연결하기 #mysql #dataviz - YouTube](https://www.youtube.com/watch?v=TFNZx5pW_t4)
* [Setup Zeppelin with K8S mode on NAVER Container Cluster | by EuiYul Song | Apr, 2021 | Medium](https://thddmlduf.medium.com/setup-zeppelin-with-k8s-mode-on-naver-container-cluster-896ca9815ca2)
* [Dynamic Forms 동작 시 랜덤하게 paragraph 내용이 사라지는 문제와 임시 해결안 | by Sinjin | Feb, 2021 | Medium](https://sinjin0.medium.com/apache-zeppelin-dynamic-forms-%EB%8F%99%EC%9E%91-%EC%8B%9C-%EB%9E%9C%EB%8D%A4%ED%95%98%EA%B2%8C-paragraph-%EB%82%B4%EC%9A%A9%EC%9D%B4-%EC%82%AC%EB%9D%BC%EC%A7%80%EB%8A%94-%EB%AC%B8%EC%A0%9C%EC%99%80-%EC%9E%84%EC%8B%9C-%ED%95%B4%EA%B2%B0%EC%95%88-da3413f16a6)
* [Incorporating Plotly into your Zeppelin notebooks with Spark and Scala](https://softwaremill.com/incorporating-plotly-into-your-zeppelin-notebooks-with-spark-and-scala/)
